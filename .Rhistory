if(length(var_names_to_qual) != 0){
# attenzione agli eventuali valori di default in ToCategoricalIncludeNA
dati[,var_names_to_qual] = apply(dati[,var_names_to_qual], 2,
function(col) ToCategoricalIncludeNA(col))
}
rm(temp_threshold)
str(dati)
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Valori unici ---------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
var_names = colnames(dati)
y_index = which(var_names == "y")
# +++++++++++++++++++++++++++++++++++++++++++++++++
# Individuo qualitative codificate come quantitative
# ++++++++++++++++++++++++++++++++++++++++++++++++++
# ottieni l'indice delle colonne delle variabili con il numero di modalità
# da eventualmente convertire in fattori
# !!!!RICHIESTA ATTENZIONE!!!!!
unique_vals_df = data.frame(nome = rep("", NCOL(dati)),
indice = rep(0, NCOL(dati)),
uniques = rep(0, NCOL(dati)))
unique_vals_df$nome = colnames(dati)
unique_vals_df$indice = as.numeric(1:NCOL(dati))
unique_vals_df$uniques = as.numeric(apply(dati, 2, function(col) length(unique(col))))
unique_vals_df
# eslcusione della y
unique_vals_df_no_y = unique_vals_df[-which(unique_vals_df$nome == "y"),]
unique_vals_df_no_y
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Riduzione categorie qualitative -------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Funzioni
# ===================================================================
# @input my_table(table)
# @input first (int): how many values frequencies one should print (default all)
# @input print_lenght (bool): print the number of total unique values
# @return values frequency in decreasing frequency order
ReturnFirstTable = function(my_table,
first = NULL,
print_length = FALSE){
if (print_length == TRUE){
print(paste("number of unique values: ", length(my_table)))
}
# print all
if(is.null(first)){
first = length(my_table)
return(my_table  %>% sort(decreasing = T))
}
# print only first
else{
return((my_table  %>% sort(decreasing = T))[1:min(first,length(my_table))])
}
}
# @input my_df (data.frame)
# @input var_index_subset (vector of int): indexes of variables subset
# @input first (int): how many values frequencies one should print (default all)
# @input print_lenght (bool): print the number of total unique values
# @print values frequency in decreasing frequency order
# going forward with the "enter" input and backward with the "b" input
PrintAllTables = function(my_df,
var_index_subset = NULL,
first = NULL,
print_length = FALSE){
# all variables
if(is.null(var_index_subset)){
var_index_subset = 1:NCOL(my_df)}
var_index_counter = 0
var_names_temp = colnames(my_df)
print("press (enter) to forward and 'b' to backward and q to quit")
while(var_index_counter < length(var_index_subset)){
input = readline("")
if((input == "q")){
var_index_counter = length(var_index_subset) - 1}
if((input != "b")){
var_index_counter = var_index_counter + 1}
if(input == "b"){
var_index_counter = var_index_counter - 1}
if(var_index_counter <= 0){
var_index_counter = 1}
print("--------------------------------------------")
print(var_names_temp[var_index_subset[var_index_counter]])
print(ReturnFirstTable(table(my_df[,var_index_subset[var_index_counter]]),
first,
print_length))
print("--------------------------------------------")}
}
# Analisi prime 40 frequenze delle modalità di tutte
# le variabili
# PrintAllTables(dati, first = 40)
# ritorna le modalità di var_name con frequenza minore di soglia
# con il "valore nuova"
# NOTA: il return DEVE essere ASSEGNATO
RaggruppaModalita = function(df, var_name, tabella_freq_modalita, soglia, valore_nuova){
# modalità al di sotto di una certa frequenza
modalita_sotto_soglia = names(which(tabella_freq_modalita < soglia))
# raggruppo queste modalità
return(ifelse(df[,var_name] %in% modalita_sotto_soglia, valore_nuova, df[,var_name]))
}
# ==========================================================================================
# Attenzione: l'implementazione di tree NON permette esplicative
# categoriali con più di 30 modalità
# quindi eventualmente ridurre a max 30 modalità
# (compromesso di perdita informazione)
# ================================================================================
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Riduzione modalità qualitative per frequenza ----------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Esempio di applicazione: cambiare il nome della variabile e la soglia
# ==============================================================================
unique_vals_df_no_y
str(dati)
# Variabile singola ----------------
# °°°°°°°°°°°°°°°°°°°° Warning: cambia nome variabile °°°°°°°°°°°°°°°°°°°°°°°°°°°°°°
# Per una specifica variabile
# temp_table_freq = TableFreqFun(dati, "x5")
# temp_table_freq
#
# dati[,"x5"] = RaggruppaModalita(dati, "x5", temp_table_freq, 400, "Altro")
#
# # check
# temp_table_freq = TableFreqFun(dati, "x5")
# temp_table_freq
#
# rm(temp_table_freq)
# Tutte le variabili character ------------------------
# per motivi computazionali, al costo di perdere informazioni
# riduco le modalità a 25 modalità
# funzione per una singola variabile
GroupValuesQual = function(df, qual_vector_var_name, new_name = "Altro"){
temp_table_freq = table(df[,qual_vector_var_name]) %>% sort(decreasing = T)
# meno di 30 modalità: non c'è bisogno di nessuna modifica
if (length(temp_table_freq) <= 25){
return(df[, qual_vector_var_name])
}
# altrimenti riduci le modalità
# seleziona la frequenza soglia oltre cui aggregare
# (temp_table_freq è già ordinata in ordine decrescente per frequenza)
freq_threshold = temp_table_freq[24]
return(RaggruppaModalita(df, qual_vector_var_name, temp_table_freq,
freq_threshold, new_name))
}
char_var_names = colnames(dati[,-y_index])[which(unlist(lapply(dati[,-y_index], typeof)) == "character")]
for(name in char_var_names){
dati[,name] = GroupValuesQual(dati, name, "Altro")
}
str(dati)
# check
unique_vals_df = data.frame(nome = rep("", NCOL(dati)),
indice = rep(0, NCOL(dati)),
uniques = rep(0, NCOL(dati)))
unique_vals_df$nome = colnames(dati)
unique_vals_df$indice = as.numeric(1:NCOL(dati))
unique_vals_df$uniques = as.numeric(apply(dati, 2, function(col) length(unique(col))))
unique_vals_df
# eslcusione della y
unique_vals_df_no_y = unique_vals_df[-which(unique_vals_df$nome == "y"),]
unique_vals_df_no_y
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Riduzione quantitative in qualitative per poche modalità --------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# + qualitative che sono codificate numericamente
# indici delle esplicative con meno di min_modalità modalità
# da aumentare in base al problema
min_modalita = 2
index_min_modalita = unique_vals_df_no_y$indice[which(unique_vals_df_no_y$uniques <= min_modalita)]
index_min_modalita
# trasformo in fattore queste ultime
for(i in index_min_modalita){
dati[,i] = as.factor(dati[,i])
}
str(dati)
# +++++++++++++++++++++++++++++++++++++++++++++++++
# Nomi e indici di colonna delle variabili
# ++++++++++++++++++++++++++++++++++++++++++++++++++
# nomi delle esplicative qualitative e quantitative
# potrei dover effettuare questa operazione più volte
y_index = which(colnames(dati) == "y")
var_factor_index = which(sapply(dati, is.factor))
# se comprende l'indice della y  lo rimuovo
# da sistemare
if (y_index %in% var_factor_index){
var_factor_index = var_factor_index[-which(var_factor_index == y_index)]}
var_char_index = which(sapply(dati, is.character))
# se comprende l'indice della y  lo rimuovo
# da sistemare
if (y_index %in% var_char_index){
var_char_index = var_char_index[-which(var_char_index == y_index)]}
# comprende anche int
var_num_index = as.numeric(which(sapply(dati, is.numeric)))
# se comprende l'indice della y lo rimuovo
if (y_index %in% var_num_index){
var_num_index = var_num_index[-which(var_num_index == y_index)]}
# +++++++++++++++++++++++++++++++++++++++++++++++++
# Conversione character in factor
# ++++++++++++++++++++++++++++++++++++++++++++++++++
for(i in var_char_index){
dati[,i] = as.factor(dati[,i])
}
str(dati)
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Aggiorno indici qualitative e nomi qualitative e quantitative
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
var_qual_index =  as.numeric(c(var_char_index, var_factor_index))
var_qual_names = var_names[var_qual_index]
var_num_names = var_names[var_num_index]
# check
var_qual_index
var_num_index
var_qual_names
var_num_names
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Analisi istrogramma quantitative -------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# per ogni variabile esplicativa quantitativa
# disegna l'istrogramma della sua distribuzione empirica
# e quella del suo logaritmo (opportunamente traslata)
# @input: my_df (data.frame)
# @input var_index_subset (vector of int): indexes of quantitative variables subset
# output: plots of each quantitative variable histogram
DrawQuantHist = function(my_df,
var_index_subset = NULL,
my_breaks = 50){
# all variables
if(is.null(var_index_subset)){
var_index_subset = 1:NCOL(my_df)}
var_index_counter = 0
var_names_temp = colnames(my_df)
par(mfrow = c(1,2))
print("press (enter) to forward and 'b' to backward and q to quit")
while(var_index_counter < length(var_index_subset)){
input = readline("")
if((input == "q")){
var_index_counter = length(var_index_subset) - 1}
if((input != "b")){
var_index_counter = var_index_counter + 1}
if(input == "b"){
var_index_counter = var_index_counter - 1}
if(var_index_counter <= 0){
var_index_counter = 1}
# original scale
hist(my_df[,var_index_subset[var_index_counter]],
breaks = my_breaks,
main = var_names_temp[var_index_subset[var_index_counter]],
xlab = "values")
# log translated scale
temp_min = min(my_df[,var_index_subset[var_index_counter]])
if(temp_min > 0){
temp_min = 0}
hist(log(my_df[,var_index_subset[var_index_counter]] - temp_min + 1e-05 ),
breaks = my_breaks,
main = paste("log", var_names_temp[var_index_subset[var_index_counter]]),
xlab = "log values")
}
par(mfrow = c(1,1))
}
# Analisi istogrammi
# DrawQuantHist(dati, var_num_index)
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Scope ----------------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# funzione per creare le stringhe di interazione
# tra variabili della stessa tipologia
# (quantitativa - quantitativa e fattore - fattore)
# '@ input: array of strings
# '@ return string formula of interaction terms
# example :
# input = c("a", "b", "c")
# output = "a:b + a:c + b:c"
MakeSameInteractionsString = function(input_var_type_names){
# preliminary checks
if(length(input_var_type_names) == 0){
cat("Warning: input_var_type_names is of length 0, return empty string")
return("")
}
type_type_interactions_string = ""
for (i in 1:length(input_var_type_names)){
for (j in (i+1):length(input_var_type_names)){
if (!(is.na(input_var_type_names[i]) | is.na(input_var_type_names[j])) & (j != i))
type_type_interactions_string = paste(type_type_interactions_string,
" + ",
input_var_type_names[i],
":",
input_var_type_names[j])
}
}
# Remove the first " + " from the string
type_type_interactions_string = substring(type_type_interactions_string, 6)
return(type_type_interactions_string)
}
# stringhe intermedie
no_interaction_string = paste(var_names[-y_index], collapse = " + ")
qual_num_interactions_string = paste(outer(var_num_names,
var_qual_names,
FUN = function(x, y) paste(x, y, sep = ":")), collapse = " + ")
qual_qual_interactions_string = MakeSameInteractionsString(var_qual_names)
num_num_interactions_string = MakeSameInteractionsString(var_num_names)
# variabili quantitative al quadrato
num_vars_square_string = ""
if(length(var_num_names) != 0){
num_vars_square_string <- paste("I(",
var_num_names,
"^2)",
sep = "", collapse = " + ")}
# string terms vector: vector of string terms
# return formula object
MakeFormula = function(string_terms_vector, intercept_bool = TRUE){
base_formula = "y ~ "
# remove empty vector terms
string_terms_vector = string_terms_vector[which(string_terms_vector != "")]
if (intercept_bool == FALSE){
base_formula = paste(base_formula, " - 1 + ")
}
added_terms = paste(string_terms_vector, collapse = " + ")
return(as.formula(paste(base_formula, added_terms)))
}
# creazione delle formule
# per evitare errori dovuti a formule troppo lunghe
options(expressions = 50000)
formula_yes_interaction_yes_intercept <- MakeFormula(c(no_interaction_string,
num_vars_square_string,
qual_qual_interactions_string,
qual_num_interactions_string))
formula_yes_interaction_no_intercept <- MakeFormula(c(no_interaction_string,
num_vars_square_string,
qual_qual_interactions_string,
qual_num_interactions_string),
intercept_bool = FALSE)
formula_yes_interaction_yes_intercept
formula_yes_interaction_no_intercept
# formula senza interazioni
formula_no_interaction_yes_intercept = MakeFormula(no_interaction_string)
formula_no_interaction_no_intercept = MakeFormula(no_interaction_string, intercept_bool = FALSE)
formula_no_interaction_yes_intercept
formula_no_interaction_no_intercept
# /////////////////////////////////////////
# Backup data.frame + environment ---------
# ////////////////////////////////////////
save(dati,
y_index,
var_qual_index, var_qual_names,
var_num_index, var_num_names,
formula_no_interaction_no_intercept,
formula_no_interaction_yes_intercept,
formula_yes_interaction_no_intercept,
formula_yes_interaction_yes_intercept,
FIGURES_FOLDER_RELATIVE_PATH,
MODELS_FOLDER_RELATIVE_PATH,
file = "result_preprocessing.Rdata")
# if necessary delete all
# rm(list = ls())
# in case of problems: load only useful objects
# load("result_preprocessing.Rdata")
# ///////////////////////////////////
# Save output on file ---------------
# //////////////////////////////////
# text.txt -------------
# # close previoulsy opened sink (if opened) -> I should make a control
# sink()
# initialize the output .txt file to regularly write on in case
# the software crashes
# open new sink
TEXT_OUTPUT_FILE_NAME = "text_output_models.txt"
# open sink
sink(TEXT_OUTPUT_FILE_NAME, append = TRUE, split = TRUE)
library(dplyr)
# parallel
library(snowfall)
# Descrizione -----------------------
# pochi dati: usiamo convalida incrociata come criterio per confrontare i modelli finali
# partizioniamo i dati in K insiemi (FOLD) e usiamo la procedura di convalida incrociata
# sia per identificare i valori ottimali dei parametri (rispetto all'errore di previsione)
# che per confrontare i migliori modelli scelti (per tutti i modelli i FOLD di convalida sono gli stessi).
# NOTA: questa procedura può risultare troppo ottimista in quanto il confronto tra i modelli migliori
# avviene rispetto all'errore in corrispondenza del parametro selezionato:
# 1) Tramite CV seleziono il parametro ottimale: quello che minimizza l'errore medio di convalida
# 2) Per ogni modello finale così selezionato confronto tali errori medi convalida
# ma è un compromesso data la scarsità dei dati
#////////////////////////////////////////////////////////////////////////////
# Costruzione metrica di valutazione e relativo dataframe -------------------
#////////////////////////////////////////////////////////////////////////////
source("loss_functions.R")
# in generale uso sia MAE che MSE
USED.Metrics = function(y.pred, y.test, weights = 1){
return(c(MAE.Loss(y.pred, y.test, weights), MSE.Loss(y.pred, y.test, weights)))
}
# anche qua
df_metrics = data.frame(name = NA, MAE = NA, MSE = NA)
METRICS_NAMES = colnames(df_metrics[,-1])
N_METRICS = length(METRICS_NAMES)
# names used to extract the metric added to df_metrics
# change based on the spefific problem
METRIC_VALUES_NAME = "metric_values"
METRIC_CHOSEN_NAME = "MSE"
# names used for accessing list CV matrix (actual metrics and metrics se)
LIST_METRICS_ACCESS_NAME = "metrics"
LIST_SD_ACCESS_NAME = "se"
# metrics names + USED.Loss
# WARNING: the order should be same as in df_metrics
MY_USED_METRICS = c("USED.Metrics", "MAE.Loss", "MSE.Loss")
MY_WEIGHTS = rep(1, nrow(dati))
#////////////////////////////////////////////////////////////////////////////
# Costruzione ID Fold convalida incrociata  -------------------
#////////////////////////////////////////////////////////////////////////////
# numero fold
K_FOLDS = 10
NROW_DF = NROW(dati)
# matrice degli id dei fold della convalida incrociata
# NOTA: data la non garantita perfetta divisibilità del numero di osservazioni
# per il numero di fold è possibile che un fold abbia meno osservazioni degli altri
# ordine casuale degli id
SHUFFLED_ID = sample(1:NROW_DF, NROW_DF)
id_matrix_cv = matrix(SHUFFLED_ID, ncol = K_FOLDS)
# converto la matrice in lista per poter avere degli elementi
# (vettori) con un diverso numero di osservazioni
# ogni colonna diventa un elemento della lista
ID_CV_LIST = list()
for(j in 1:ncol(id_matrix_cv)){
ID_CV_LIST[[j]] = id_matrix_cv[,j]
}
rm(id_matrix_cv)
gc()
# se ottengo Warning: non divisibilità perfetta
# significa che l'ultimo elemento lista contiene
# degli id che sono presenti anche nel primo elemento
# sistemo eliminando dall'ultimo elemento della lista gli id presenti anche nel primo elemento
# controllo il resto della divisione
integer_division_cv = NROW_DF %/% K_FOLDS
modulo_cv = NROW_DF %% K_FOLDS
if(modulo_cv != 0){
ID_CV_LIST[[K_FOLDS]] = ID_CV_LIST[[K_FOLDS]][1:integer_division_cv]
}
# /////////////////////////////////////////////////////////////////
#------------------------ Modelli ---------------------------------
# /////////////////////////////////////////////////////////////////
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Media e Mediana --------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# aggiunta media e mediana della risposta sull'insieme di stima come possibili modelli
# (per valutare se modelli più complessi hanno senso)
# in questo caso non ci sono parametri: solo fold (righe) e metriche (2: MSE e MAE)
# media
temp_err_matrix_cv = matrix(NA, nrow = K_FOLDS, ncol = N_METRICS)
colnames(temp_err_matrix_cv) = colnames(df_metrics[,-1])
for (i in 1:K_FOLDS){
temp_err_matrix_cv[i,] = USED.Metrics(mean(dati$y[unlist(ID_CV_LIST[-i])]),
dati$y[ID_CV_LIST[[i]]])
}
df_metrics = Add_Test_Metric(df_metrics,
"cv mean",
colMeans(temp_err_matrix_cv))
# mediana
temp_err_matrix_cv = matrix(NA, nrow = K_FOLDS, ncol = N_METRICS)
for (i in 1:K_FOLDS){
temp_err_matrix_cv[i,] = USED.Metrics(median(dati$y[unlist(ID_CV_LIST[-i])]),
dati$y[ID_CV_LIST[[i]]])
}
df_metrics = Add_Test_Metric(df_metrics,
"cv median",
colMeans(temp_err_matrix_cv))
df_metrics = na.omit(df_metrics)
df_metrics
# max number of ridge functions
PPR_MAX_RIDGE_FUNCTIONS = 4
# possible spline degrees of freedom
PPR_DF_SM = 2:6
ppr_metrics = PPRRegulationCV(my_data = dati,
my_id_list_cv_train = ID_CV_LIST,
my_max_ridge_functions = PPR_MAX_RIDGE_FUNCTIONS,
my_spline_df = PPR_DF_SM,
my_metrics_names = METRICS_NAMES,
my_weights = MY_WEIGHTS,
use_only_first_fold = USE_ONLY_FIRST_FOLD,
is_classification = FALSE)
source("cv_functions.R")
# max number of ridge functions
PPR_MAX_RIDGE_FUNCTIONS = 4
# possible spline degrees of freedom
PPR_DF_SM = 2:6
ppr_metrics = PPRRegulationCV(my_data = dati,
my_id_list_cv_train = ID_CV_LIST,
my_max_ridge_functions = PPR_MAX_RIDGE_FUNCTIONS,
my_spline_df = PPR_DF_SM,
my_metrics_names = METRICS_NAMES,
my_weights = MY_WEIGHTS,
use_only_first_fold = USE_ONLY_FIRST_FOLD,
is_classification = FALSE)
ppr_metrics = PPRRegulationCV(my_data = dati,
my_id_list_cv_train = ID_CV_LIST,
my_max_ridge_functions = PPR_MAX_RIDGE_FUNCTIONS,
my_spline_df = PPR_DF_SM,
my_metrics_names = METRICS_NAMES,
my_weights = MY_WEIGHTS,
is_classification = FALSE)
ppr_best_params = PPRExtractBestParams(ppr_metrics)
ppr_n_ridges_best = ppr_best_params[[METRIC_CHOSEN_NAME]][[1]]
ppr_df_best = ppr_best_params[[METRIC_CHOSEN_NAME]][[2]]
print("ppr best params")
ppr_best_params
ppr_metrics = PPRRegulationCVParallel(my_data = dati,
my_id_list_cv_train = ID_CV_LIST,
my_max_ridge_functions = PPR_MAX_RIDGE_FUNCTIONS,
my_spline_df = PPR_DF_SM,
my_metrics_names = METRICS_NAMES,
my_weights = MY_WEIGHTS,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES,
is_classification = FALSE)
ppr_best_params = PPRExtractBestParams(ppr_metrics)
ppr_n_ridges_best = ppr_best_params[[METRIC_CHOSEN_NAME]][[1]]
ppr_df_best = ppr_best_params[[METRIC_CHOSEN_NAME]][[2]]
print("ppr best params")
ppr_best_params
