id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in my_n_bs_trees){
temp_rf = randomForest(y ~., data = my_data[id_train],
mtry = my_n_variables, ntree = t)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 200,
fix_trees_bool = TRUE)
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the the vector (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector (supposed to be a sequence)
#'
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
FewDataCVCycleRF = function(n_k_fold, my_id_list_cv,my_n_metrics,
my_metric_names, my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 1:RF_N_BS_TREES,
fix_trees_bool = TRUE){
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in my_n_bs_trees){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = my_n_variables, ntree = t)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 200,
fix_trees_bool = TRUE)
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the the vector (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector (supposed to be a sequence)
#'
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
FewDataCVCycleRF = function(n_k_fold, my_id_list_cv,my_n_metrics,
my_metric_names, my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 1:RF_N_BS_TREES,
fix_trees_bool = TRUE){
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in my_n_bs_trees){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = my_n_variables, ntree = t)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 200,
fix_trees_bool = TRUE)
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the the vector (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector (supposed to be a sequence)
#'
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
FewDataCVCycleRF = function(n_k_fold, my_id_list_cv,my_n_metrics,
my_metric_names, my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 1:RF_N_BS_TREES,
fix_trees_bool = TRUE){
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_err_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in my_n_bs_trees){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = my_n_variables, ntree = t)
# prediction error
temp_metrics_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 200,
fix_trees_bool = TRUE)
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the the vector (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector (supposed to be a sequence)
#'
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
FewDataCVCycleRF = function(n_k_fold, my_id_list_cv,my_n_metrics,
my_metric_names, my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 1:RF_N_BS_TREES,
fix_trees_bool = TRUE){
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_metrics_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in my_n_bs_trees){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = my_n_variables, ntree = t)
# prediction error
temp_metrics_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 200,
fix_trees_bool = TRUE)
rf_best_summary = CvMetricPlotMin(my_param_values = 1:RF_MAX_VARIABLES,
my_metric_matrix = rf_cv_metrics,
my_metric_names = METRICS_NAMES,
my_main = "RF CV error",
my_xlab = "Number of variables at each split",
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"rf_metrics_plot.jpeg"))
rf_best_summary
rf_cv_metrics_bts_trees = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = rf_best_summary[[METRIC_CHOSEN_NAME]][[METRIC_VALUES_NAME]],
my_n_bs_trees = seq(30, 400, 20),
fix_trees_bool = FALSE)
rf_best_summary[[METRIC_CHOSEN_NAME]][[METRIC_VALUES_NAME]]
rf_cv_metrics_bts_trees = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = rf_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]],
my_n_bs_trees = seq(30, 400, 20),
fix_trees_bool = FALSE)
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the the vector (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector (supposed to be a sequence)
#'
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
FewDataCVCycleRF = function(n_k_fold, my_id_list_cv,my_n_metrics,
my_metric_names, my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 1:RF_N_BS_TREES,
fix_trees_bool = TRUE){
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_metrics_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in my_n_bs_trees){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = my_n_variables, ntree = t)
# prediction error
temp_metrics_array_cv[k,t,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics_bts_trees = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = rf_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]],
my_n_bs_trees = seq(30, 400, 20),
fix_trees_bool = FALSE)
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the the vector (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector (supposed to be a sequence)
#'
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
FewDataCVCycleRF = function(n_k_fold, my_id_list_cv,my_n_metrics,
my_metric_names, my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 1:RF_N_BS_TREES,
fix_trees_bool = TRUE){
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
if(fix_trees_bool == TRUE){
for (m in my_n_variables){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = m, ntree = my_n_bs_trees)
# prediction error
temp_metrics_array_cv[k,m,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
else{
for (t in 1:tuning_parameter_length){
temp_rf = randomForest(y ~., data = my_data[id_train,],
mtry = my_n_variables, ntree = my_n_bs_trees[t])
# prediction error
temp_metrics_array_cv[k,t,] = USED.Loss(predict(temp_rf, my_data[id_test,]),
my_data$y[id_test])
}
}
rm(temp_rf)
gc()
print(paste("fold ", k))
}
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
}
return(cv_metrics)
}
rf_cv_metrics_bts_trees = FewDataCVCycleRF(n_k_fold = K_FOLD,
my_id_list_cv = id_list_cv,
my_n_metrics = N_METRICS_df_metrics,
my_metric_names = METRICS_NAMES,
my_data = dati,
my_n_variables = rf_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]],
my_n_bs_trees = seq(30, 400, 20),
fix_trees_bool = FALSE)
rf_best_summary_bts_trees = CvMetricPlotMin(my_param_values = seq(30, 400, 20),
my_metric_matrix = rf_cv_metrics,
my_metric_names = METRICS_NAMES,
my_main = "RF CV error",
my_xlab = "Number of bootstrap tress",
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"rf_metrics_plot_bts.jpeg"))
rf_cv_metrics_bts_trees
seq(30, 400, 20)
rf_best_summary_bts_trees = CvMetricPlotMin(my_param_values = seq(30, 400, 20),
my_metric_matrix = rf_cv_metrics_bts_trees,
my_metric_names = METRICS_NAMES,
my_main = "RF CV error",
my_xlab = "Number of bootstrap tress",
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"rf_metrics_plot_bts.jpeg"))
