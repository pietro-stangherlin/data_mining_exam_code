# al logaritmo del rapporto
df_metrics = Add_Test_Metric(df_metrics,
"sss mean",
USED.Metrics(mean(sss$y), vvv$y))
df_metrics = Add_Test_Metric(df_metrics,
"sss median",
USED.Metrics(median(sss$y), vvv$y))
df_metrics = na.omit(df_metrics)
df_metrics
# default: molto fitto
tree_full = tree(y ~.,
data = sss[id_cb1,],
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
minsize = 5))
# controllo che sia sovraadattato
plot(tree_full)
# potatura
tree_pruned = prune.tree(tree_full, newdata = sss[-id_cb1,])
plot(tree_pruned)
plot(tree_pruned, xlim = c(0, 20))
tree_best_size = tree_pruned$size[which.min(tree_pruned$dev)]
print("tree best size")
tree_best_size
plot(tree_pruned, xlim = c(0, 40))
tree_best_size = tree_pruned$size[which.min(tree_pruned$dev)]
print("tree best size")
tree_best_size
abline(v = tree_best_size, col = "red")
final_tree_pruned = prune.tree(tree_full, best = tree_best_size)
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
plot(final_tree_pruned)
final_tree_pruned = prune.tree(tree_full,
best = tree_best_size,
newdata = sss[-id_cb1,])
plot(final_tree_pruned)
?prune.tree
final_tree_pruned = prune.tree(tree_full,
best = tree_best_size,
newdata = sss[1:30000,])
plot(final_tree_pruned)
final_tree_pruned = prune.tree(tree_full,
best = tree_best_size,
newdata = sss[1:300,])
plot(final_tree_pruned)
tree_best_size
final_tree_pruned = prune.tree(tree_full,
best = 2,
newdata = sss[1:300,])
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
final_tree_pruned = prune.tree(tree_full,
best = 2,
newdata = sss[1:10000,])
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
final_tree_pruned = prune.tree(tree_full,
best = 4,
newdata = sss[1:10000,])
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
l
final_tree_pruned = prune.tree(tree_full,
best = 4,
newdata = sss[30000:40000,])
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
# potatura
tree_pruned = prune.tree(tree_full,
newdata = sss[-id_cb1,],
nwts = MY_WEIGHTS)
r
# potatura
tree_pruned = prune.tree(tree_full,
newdata = sss[-id_cb1,],
nwts = MY_WEIGHTS)
plot(tree_pruned)
MY_WEIGHTS
tree_best_size = tree_pruned$size[which.min(tree_pruned$dev)]
print("tree best size")
tree_best_size
# potatura
tree_pruned = prune.tree(tree_full,
newdata = sss[-id_cb1,])
plot(tree_pruned)
plot(tree_pruned, xlim = c(0, 40))
tree_best_size = tree_pruned$size[which.min(tree_pruned$dev)]
print("tree best size")
tree_best_size
TREE_MAX_SIZE = 100
# if parallel shows problems use the non parallel version
tree_cv_metrics = ManualCvTreeParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES,
my_weights = MY_WEIGHTS,
my_mindev = 1e-04,
my_minsize = 5)
tree_best_summary = CvMetricBest(my_param_values = 2:TREE_MAX_SIZE,
my_metric_matrix = tree_cv_metrics[["metrics"]],
my_one_se_best = TRUE,
my_higher_more_complex = TRUE,
my_se_matrix = tree_cv_metrics[["se"]],
my_metric_names = METRICS_NAMES)
temp_plot_function = function(){
PlotCvMetrics(my_param_values = 2:TREE_MAX_SIZE,
my_metric_matrix = tree_cv_metrics[["metrics"]],
my_se_matrix = tree_cv_metrics[["se"]],
my_best_param_values = ExtractBestParams(tree_best_summary),
my_metric_names = METRICS_NAMES,
my_main = "Tree CV metrics",
my_xlab = "size")
}
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"tree_cv_metrics_plot.jpeg",
collapse = ""))
tree_best_size = tree_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]]
print("tree best size")
tree_best_size
tree_pruned$size
temp_plot_function = function(){
plot(tree_pruned)
plot(tree_pruned, xlim = c(0, 40))
abline(v = tree_best_size, col = "red")
}
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"tree_test_deviance_plot.jpeg",
collapse = ""))
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"tree_test_deviance_plot.jpeg",
collapse = ""))
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"tree_test_deviance_plot.jpeg",
collapse = ""))
final_tree_pruned = prune.tree(tree_full,
best = tree_best_size)
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
df_metrics = Add_Test_Metric(df_metrics,
"tree_pruned best",
USED.Metrics(predict(final_tree_pruned, newdata = vvv), vvv$y))
df_metrics
file_name_final_tree_pruned = paste(MODELS_FOLDER_RELATIVE_PATH,
"final_tree_pruned",
".Rdata", collapse = "", sep = "")
save(final_tree_pruned, file = file_name_final_tree_pruned)
rm(final_tree_pruned)
rm(tree_full)
gc()
unlist(df_metrics)
# °°°°°°°°°°°°°°°°°°°°°°°°°°°Warning: lento°°°°°°°°°°°°°°°°°°°°°°°°°°°°
# stepwise forward: AIC based on generalized df
gam0 = gam(y ~ 1, data = sss)
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Modello Additivo ---------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
library(gam)
# °°°°°°°°°°°°°°°°°°°°°°°°°°°Warning: lento°°°°°°°°°°°°°°°°°°°°°°°°°°°°
# stepwise forward: AIC based on generalized df
gam0 = gam(y ~ 1, data = sss)
# riconosce le qualitative se sono fattori
my_gam_scope = gam.scope(sss[,-y_index], arg = c("df=2", "df=3", "df=4", "df=5", "df=6"))
gam_step = step.Gam(gam0, scope = my_gam_scope)
object.size(gam_step)
df_metrics = Add_Test_Metric(df_metrics,
"gam_step",
USED.Metrics(predict(gam_step, newdata = vvv), vvv$y))
df_metrics
file_name_gam_step = paste(MODELS_FOLDER_RELATIVE_PATH,
"gam_step",
".Rdata", collapse = "", sep = "")
save(gam_step, file = file_name_gam_step)
rm(gam_step)
gc()
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
colnames(X_mm_no_interaction_sss)
# valuta: se ci sono molte esplicative qualitative -> model.matrix con molti zeri
library(Matrix)
X_mm_no_interaction_sss =  sparse.model.matrix(formula_no_interaction_no_intercept, data = sss)
X_mm_no_interaction_vvv =  sparse.model.matrix(formula_no_interaction_no_intercept, data = vvv)
colnames(X_mm_no_interaction_sss)
# devo ottenere gli indici delle colonne
# delle variabili qualitative della matrice del disegno (senza intercetta)
# poichè trasformando il data.frame in matrice del modello
# i nomi delle variabili quantitative rimangono invariati
# selezioniamo prima quest'ultime
num_index = which(colnames(X_mm_no_interaction_sss) %in% var_num_names)
factor_index = setdiff(1:NCOL(X_mm_no_interaction_sss), num_index)
num_index
factor_index
library(polspline)
mars1 = polymars(responses = sss$y,
predictors = X_mm_no_interaction_sss,
gcv = 1,
factors = factor_index,
maxsize = 60)
# pas
mars1$fitting
print("mars min size gcv")
min_size_mars
min_size_mars = mars1$fitting$size[which.min(mars1$fitting$GCV)]
min_size_mars
temp_plot_function = function(){
plot(mars1$fitting$size, mars1$fitting$GCV,
col = as.factor(mars1$fitting$`0/1`),
pch = 16,
xlab = "numero di basi",
ylab = "GCV")
legend(c("topright"),
legend = c("crescita", "potatura"),
col = c("black","red"),
pch = 16)
abline(v = min_size_mars)
}
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"mars_gcv_plot.jpeg",
collapse = ""))
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"mars_gcv_plot.jpeg",
collapse = ""))
temp_plot_function = function(){
plot(mars1$fitting$size, mars1$fitting$GCV,
col = as.factor(mars1$fitting$`0/1`),
pch = 16,
xlab = "basis number",
ylab = "GCV",
main = "MARS step GCV")
legend(c("topright"),
legend = c("crescita", "potatura"),
col = c("black","red"),
pch = 16)
abline(v = min_size_mars)
}
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"mars_gcv_plot.jpeg",
collapse = ""))
?polymars
object.size(mars1)
df_metrics = Add_Test_Metric(df_metrics,
"MARS",
USED.Metrics(predict(mars1, x = X_mm_no_interaction_vvv),vvv$y))
df_metrics
df_metrics = Add_Test_Metric(df_metrics,
"MARS",
USED.Metrics(predict(mars_step, x = X_mm_no_interaction_vvv),vvv$y))
df_metrics
file_name_mars_step = paste(MODELS_FOLDER_RELATIVE_PATH,
"mars_step",
".Rdata", collapse = "", sep = "")
save(mars_step, file = file_name_mars_step)
mars_step = polymars(responses = sss$y,
predictors = X_mm_no_interaction_sss,
gcv = 1,
factors = factor_index,
maxsize = 60)
print("mars min size gcv")
min_size_mars = mars_step$fitting$size[which.min(mars_step$fitting$GCV)]
min_size_mars
temp_plot_function = function(){
plot(mars_step$fitting$size, mars_step$fitting$GCV,
col = as.factor(mars_step$fitting$`0/1`),
pch = 16,
xlab = "basis number",
ylab = "GCV",
main = "MARS step GCV")
legend(c("topright"),
legend = c("crescita", "potatura"),
col = c("black","red"),
pch = 16)
abline(v = min_size_mars)
}
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"mars_gcv_plot.jpeg",
collapse = ""))
df_metrics = Add_Test_Metric(df_metrics,
"MARS",
USED.Metrics(predict(mars_step, x = X_mm_no_interaction_vvv),vvv$y))
df_metrics
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
file_name_mars_step = paste(MODELS_FOLDER_RELATIVE_PATH,
"mars_step",
".Rdata", collapse = "", sep = "")
save(mars_step, file = file_name_mars_step)
rm(mars_step)
gc()
# PPR CV function
#' @param my_id_list_cv (list):ids in each fold , use the global variable
#' @param my_metric_names (vector of string): ordered names of loss functions, use global variables
#' @param my_data (data.frame): data.frame used
#'
#' @param my_max_ridges (int): max number of ridge functions
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvPPR = function(my_id_list_cv,
my_metric_names,
my_data,
my_max_ridges = PPR_MAX_RIDGE_FUNCTIONS,
my_weights = MY_WEIGHTS){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, my_max_ridges, my_n_metrics))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# cycle through different numbers of ridge functions
for (r in 1:my_max_ridges){
temp_ppr = ppr(y ~ .,
data = my_data[id_train,],
nterms = r)
# prediction error
temp_metrics_array_cv[k,r,] = USED.Metrics(predict(temp_ppr, my_data[id_test,]),
my_data$y[id_test],
weights = my_weights)
}
rm(temp_ppr)
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = my_max_ridges, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = my_max_ridges, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
# PPR CV function
#' @param my_id_list_cv (list):ids in each fold , use the global variable
#' @param my_metric_names (vector of string): ordered names of loss functions, use global variables
#' @param my_data (data.frame): data.frame used
#'
#' @param my_max_ridges (int): max number of ridge functions
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvPPRParallel = function(my_id_list_cv,
my_metric_names,
my_data,
my_max_ridges = PPR_MAX_RIDGE_FUNCTIONS,
my_weights = 1,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, my_max_ridges, my_n_metrics))
sfInit(cpus =my_ncores, parallel = T)
sfExport(list = c("my_data", my_metrics_functions, "my_max_ridges", "my_weights"))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
sfExport(list = c("id_train", "id_test"))
# for better readability
temp_metric = sfLapply(1:my_max_ridges,
fun = function(r)
USED.Metrics(predict(ppr(y ~ .,
data = my_data[id_train,],
nterms = r),
my_data[id_test,]), my_data$y[id_test]))
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,,] = matrix(unlist(temp_metric), ncol = my_n_metrics, byrow = T)
rm(temp_metric)
# cycle through different numbers of ridge functions
for (r in 1:my_max_ridges){
temp_ppr = ppr(y ~ .,
data = my_data[id_train,],
nterms = r)
# prediction error
temp_metrics_array_cv[k,r,] = USED.Metrics(predict(temp_ppr, my_data[id_test,]),
my_data$y[id_test],
weights = my_weights)
}
rm(temp_ppr)
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = my_max_ridges, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = my_max_ridges, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
ppr_best_n_ridges = which.min(err_ppr_test_validation)
# numero di possibili funzioni dorsali
PPR_MAX_RIDGE_FUNCTIONS = 4
err_ppr_test_validation = rep(NA, PPR_MAX_RIDGE_FUNCTIONS)
for(k in 1:PPR_MAX_RIDGE_FUNCTIONS){
mod = ppr(y ~ .,
data = sss[id_cb1,],
nterms = k)
err_ppr_test_validation[k] = MSE.Loss(predict(mod, sss[-id_cb1,]), sss$y[-id_cb1])
}
rm(mod)
print("err_ppr_test_validation")
err_ppr_test_validation
ppr_best_n_ridges = which.min(err_ppr_test_validation)
ppr_best_n_ridges
ppr_best_n_ridges
for(k in 1:PPR_MAX_RIDGE_FUNCTIONS){
mod = ppr(y ~ .,
x = X_mm_no_interaction_sss[id_cb1,],
y = sss$y[id_cb1],
nterms = k)
err_ppr_test_validation[k] = MSE.Loss(predict(mod, sss[-id_cb1,]), sss$y[-id_cb1])
print(k)
}
sm.method
for(k in 1:PPR_MAX_RIDGE_FUNCTIONS){
mod = ppr(y ~ .,
data = sss[id_cb1,],
nterms = k)
err_ppr_test_validation[k] = MSE.Loss(predict(mod, sss[-id_cb1,]), sss$y[-id_cb1])
print(k)
}
mod$df
mod$beta
# numero di possibili gradi di libertà (equivalenti) delle smoothing splines
PPR_E_DF_SM = 2:6
err_ppr_matrix = matrix(NA, nrow = PPR_MAX_RIDGE_FUNCTIONS, ncol = length(PPR_E_DF_SM))
err_ppr_matrix = matrix(NA, nrow = PPR_MAX_RIDGE_FUNCTIONS, ncol = length(PPR_E_DF_SM))
for(r in 1:PPR_MAX_RIDGE_FUNCTIONS){
for(df in 1: length(PPR_E_DF_SM)){
mod = ppr(y ~ .,
data = sss[id_cb1,],
nterms = r,
sm.method = "spline",
df = PPR_E_DF_SM[df])
}
err_ppr_matrix[r, df] = MSE.Loss(predict(mod, sss[-id_cb1,]), sss$y[-id_cb1])
print(k)
}
err_ppr_matrix
ppr_best_n_ridges
length(PPR_E_DF_SM
)
PPR_E_DF_SM[1]
PPR_E_DF_SM[4]
# numero di possibili funzioni dorsali
PPR_MAX_RIDGE_FUNCTIONS = 4
# numero di possibili gradi di libertà (equivalenti) delle smoothing splines
PPR_E_DF_SM = 2:6
err_ppr_matrix = matrix(NA, nrow = PPR_MAX_RIDGE_FUNCTIONS, ncol = length(PPR_E_DF_SM))
err_ppr_matrix
for(r in 1:PPR_MAX_RIDGE_FUNCTIONS){
for(df in 1: length(PPR_E_DF_SM)){
mod = ppr(y ~ .,
data = sss[id_cb1,],
nterms = r,
sm.method = "spline",
df = PPR_E_DF_SM[df])
err_ppr_matrix[r, df] = MSE.Loss(predict(mod, sss[-id_cb1,]), sss$y[-id_cb1])
}
print(r)
}
err_ppr_matrix
which.min(err_ppr_matrix)
err_ppr_matrix[17]
unlist(err_ppr_matrix)
as.vector(err_ppr_matrix)
which.min(as.vector(err_ppr_matrix))
matrix(1:20, nrow = 4, ncol = 5)
matrix(1:20, nrow = 4, ncol = 5, byrow = T)
matrix(1:20, nrow = 4, ncol = 5)
rm(err_ppr_matrix)
metrics_ppr_array = array(NA, dim = c(PPR_MAX_RIDGE_FUNCTIONS, length(PPR_E_DF_SM), N_METRICS))
for(r in 1:PPR_MAX_RIDGE_FUNCTIONS){
for(df in 1: length(PPR_E_DF_SM)){
mod = ppr(y ~ .,
data = sss[id_cb1,],
nterms = r,
sm.method = "spline",
df = PPR_E_DF_SM[df])
metrics_ppr_array[r, df, ] = USED.Metrics(predict(mod, sss[-id_cb1,]),
sss$y[-id_cb1],
weights = MY_WEIGHTS)
}
print(r)
}
metrics_ppr_array
colnames(metrics_ppr_array)
colnames(metrics_ppr_array[,,])
?array
dimnames(metrics_ppr_array)
METRICS_NAMES
dimnames(metrics_ppr_array) = list(NA, NA, METRICS_NAMES)
dimnames(metrics_ppr_array) = list(rep(NA, PPR_MAX_RIDGE_FUNCTIONS),
rep(NA, length(PPR_E_DF_SM)),
METRICS_NAMES)
metrics_ppr_array
dimnames(metrics_ppr_array) = list(1:PPR_MAX_RIDGE_FUNCTIONS,
1:length(PPR_E_DF_SM),
METRICS_NAMES)
metrics_ppr_array
dimnames(metrics_ppr_array) = list(1:PPR_MAX_RIDGE_FUNCTIONS,
PPR_E_DF_SM,
METRICS_NAMES)
metrics_ppr_array
