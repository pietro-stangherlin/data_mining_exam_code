var_names_to_remove
if(length(var_names_to_remove != 0)){
row_missing_index_NA_to_remove = which((apply(dati[,var_names_to_remove], 1, function(row)( NA %in% row))))
dati = dati[-row_missing_index_NA_to_remove,]}
str(dati)
# 2)
# funzione per una di queste variabili
# @param var_vector: vettore di variabile quantitativa con eventuali NA
# @param my_breaks vettore di separazione: default NA
# @param my_probs vettore di quantili: default quartili
# @return: vettore di character della variabile categorizzata per quantili
# + modalità EMPTY al posto dei NA
ToCategoricalIncludeNA = function(var_vector,
my_breaks = NA,
my_probs = seq(0, 1, 0.25)){
NOT_NA_index = which(!is.na(var_vector))
used_breaks = my_breaks
# default
if(is.na(my_breaks)){
used_breaks = quantile(var_vector, probs = my_probs, na.rm = TRUE)
}
to_breaks = as.character(cut(var_vector[NOT_NA_index],
breaks = unique(used_breaks),
include.lowest = TRUE))
returned_vector = rep("EMPTY", length(var_vector))
returned_vector[NOT_NA_index] = to_breaks
return(returned_vector)
}
# test
# ToCategoricalIncludeNA(c(1,2,NA,4,5,NA))
# lista di variabili quantitative da trasformare in qualitative
var_names_to_qual = colnames(dati[,which(relative_missing_freqs >= temp_threshold)])
var_names_to_qual
if(length(var_names_to_qual) != 0){
# attenzione agli eventuali valori di default in ToCategoricalIncludeNA
dati[,var_names_to_qual] = apply(dati[,var_names_to_qual], 2,
function(col) ToCategoricalIncludeNA(col))
}
rm(temp_threshold)
str(dati)
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Valori unici ---------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
var_names = colnames(dati)
y_index = which(var_names == "y")
# +++++++++++++++++++++++++++++++++++++++++++++++++
# Individuo qualitative codificate come quantitative
# ++++++++++++++++++++++++++++++++++++++++++++++++++
# ottieni l'indice delle colonne delle variabili con il numero di modalità
# da eventualmente convertire in fattori
# !!!!RICHIESTA ATTENZIONE!!!!!
unique_vals_df = data.frame(nome = rep("", NCOL(dati)),
indice = rep(0, NCOL(dati)),
uniques = rep(0, NCOL(dati)))
unique_vals_df$nome = colnames(dati)
unique_vals_df$indice = as.numeric(1:NCOL(dati))
unique_vals_df$uniques = as.numeric(apply(dati, 2, function(col) length(unique(col))))
unique_vals_df
# eslcusione della y
unique_vals_df_no_y = unique_vals_df[-which(unique_vals_df$nome == "y"),]
unique_vals_df_no_y
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Riduzione categorie qualitative -------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Per variabili che so già essere qualitative
# Funzioni
# ===================================================================
# ritorna la tabella con le modalità e le rispettive frequenze
# per il data.frame df e la variabile var_name
TableFreqFun = function(df, var_name){
table_modalita_decr = table(df[,var_name]) %>% sort(decreasing = T)
# quante modalità
print("Numero di modalità uniche")
print( df[,var_name] %>% unique() %>% length())
return(table_modalita_decr)
}
# ritorna le modalità di var_name con frequenza minore di soglia
# con il "valore nuova"
# NOTA: il return DEVE essere ASSEGNATO
RaggruppaModalita = function(df, var_name, tabella_freq_modalita, soglia, valore_nuova){
# modalità al di sotto di una certa frequenza
modalita_sotto_soglia = names(which(tabella_freq_modalita < soglia))
# raggruppo queste modalità
return(ifelse(df[,var_name] %in% modalita_sotto_soglia, valore_nuova, df[,var_name]))
}
# ==========================================================================================
# Attenzione: l'implementazione di tree NON permette esplicative
# categoriali con più di 30 modalità
# quindi eventualmente ridurre a max 30 modalità
# (compromesso di perdita informazione)
# ================================================================================
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Riduzione modalità qualitative per frequenza ----------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Esempio di applicazione: cambiare il nome della variabile e la soglia
# ==============================================================================
unique_vals_df_no_y
str(dati)
# Variabile singola ----------------
# °°°°°°°°°°°°°°°°°°°° Warning: cambia nome variabile °°°°°°°°°°°°°°°°°°°°°°°°°°°°°°
# Per una specifica variabile
# temp_table_freq = TableFreqFun(dati, "x5")
# temp_table_freq
#
# dati[,"x5"] = RaggruppaModalita(dati, "x5", temp_table_freq, 400, "Altro")
#
# # check
# temp_table_freq = TableFreqFun(dati, "x5")
# temp_table_freq
#
# rm(temp_table_freq)
# Tutte le variabili character ------------------------
# per motivi computazionali, al costo di perdere informazioni
# riduco le modalità a 25 modalità
# funzione per una singola variabile
GroupValuesQual = function(df, qual_vector_var_name, new_name = "Altro"){
temp_table_freq = TableFreqFun(df, qual_vector_var_name)
# meno di 30 modalità: non c'è bisogno di nessuna modifica
if (length(temp_table_freq) <= 25){
return(df[, qual_vector_var_name])
}
# altrimenti riduci le modalità
# seleziona la frequenza soglia oltre cui aggregare
# (temp_table_freq è già ordinata in ordine decrescente per frequenza)
freq_threshold = temp_table_freq[24]
return(RaggruppaModalita(df, qual_vector_var_name, temp_table_freq,
freq_threshold, new_name))
}
char_var_names = colnames(dati[,-y_index])[which(unlist(lapply(dati[,-y_index], typeof)) == "character")]
for(name in char_var_names){
dati[,name] = GroupValuesQual(dati, name, "Altro")
}
str(dati)
# check
unique_vals_df = data.frame(nome = rep("", NCOL(dati)),
indice = rep(0, NCOL(dati)),
uniques = rep(0, NCOL(dati)))
unique_vals_df$nome = colnames(dati)
unique_vals_df$indice = as.numeric(1:NCOL(dati))
unique_vals_df$uniques = as.numeric(apply(dati, 2, function(col) length(unique(col))))
unique_vals_df
# eslcusione della y
unique_vals_df_no_y = unique_vals_df[-which(unique_vals_df$nome == "y"),]
unique_vals_df_no_y
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Riduzione quantitative in qualitative per poche modalità --------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# + qualitative che sono codificate numericamente
# indici delle esplicative con meno di min_modalità modalità
# da aumentare in base al problema
min_modalita = 2
index_min_modalita = unique_vals_df_no_y$indice[which(unique_vals_df_no_y$uniques <= min_modalita)]
index_min_modalita
# trasformo in fattore queste ultime
for(i in index_min_modalita){
dati[,i] = as.factor(dati[,i])
}
str(dati)
# +++++++++++++++++++++++++++++++++++++++++++++++++
# Nomi e indici di colonna delle variabili
# ++++++++++++++++++++++++++++++++++++++++++++++++++
# nomi delle esplicative qualitative e quantitative
# potrei dover effettuare questa operazione più volte
var_factor_index = which(sapply(dati, is.factor))
# se comprende l'indice della y  lo rimuovo
# da sistemare
if (y_index %in% var_factor_index){
var_factor_index = var_factor_index[-which(var_factor_index == y_index)]}
var_char_index = which(sapply(dati, is.character))
# se comprende l'indice della y  lo rimuovo
# da sistemare
if (y_index %in% var_char_index){
var_char_index = var_char_index[-which(var_char_index == y_index)]}
# comprende anche int
var_num_index = as.numeric(which(sapply(dati, is.numeric)))
# se comprende l'indice della y lo rimuovo
if (y_index %in% var_num_index){
var_num_index = var_num_index[-which(var_num_index == y_index)]}
# +++++++++++++++++++++++++++++++++++++++++++++++++
# Conversione character in factor
# ++++++++++++++++++++++++++++++++++++++++++++++++++
for(i in var_char_index){
dati[,i] = as.factor(dati[,i])
}
str(dati)
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# Aggiorno indici qualitative e nomi qualitative e quantitative
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
var_qual_index =  as.numeric(c(var_char_index, var_factor_index))
var_qual_names = var_names[var_qual_index]
var_num_names = var_names[var_num_index]
# check
var_qual_index
var_num_index
var_qual_names
var_num_names
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Scope ----------------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Attenzione: solo per regressione o classificazione binaria
# funzione per creare le stringhe di interazione
# tra variabili della stessa tipologia
# (quantitativa - quantitativa e fattore - fattore)
# '@ input: array of strings
# '@ return string formula of interaction terms
# example :
# input = c("a", "b", "c")
# output = "a:b + a:c + b:c"
MakeSameInteractionsString = function(input_var_type_names){
# preliminary checks
if(length(input_var_type_names) == 0){
cat("Warning: input_var_type_names is of length 0, return empty string")
return("")
}
type_type_interactions_string = ""
for (i in 1:length(input_var_type_names)){
for (j in (i+1):length(input_var_type_names)){
if (!(is.na(input_var_type_names[i]) | is.na(input_var_type_names[j])) & (j != i))
type_type_interactions_string = paste(type_type_interactions_string,
" + ",
input_var_type_names[i],
":",
input_var_type_names[j])
}
}
# Remove the first " + " from the string
type_type_interactions_string = substring(type_type_interactions_string, 6)
return(type_type_interactions_string)
}
# stringhe intermedie
no_interaction_string = paste(var_names[-y_index], collapse = " + ")
qual_num_interactions_string = paste(outer(var_num_names,
var_qual_names,
FUN = function(x, y) paste(x, y, sep = ":")), collapse = " + ")
qual_qual_interactions_string = MakeSameInteractionsString(var_qual_names)
num_num_interactions_string = MakeSameInteractionsString(var_num_names)
# variabili quantitative al quadrato
num_vars_square_string = ""
if(length(var_num_names) != 0){
num_vars_square_string <- paste("I(",
var_num_names,
"^2)",
sep = "", collapse = " + ")}
# string terms vector: vector of string terms
# return formula object
MakeFormula = function(string_terms_vector, intercept_bool = TRUE){
base_formula = "y ~ "
# remove empty vector terms
string_terms_vector = string_terms_vector[which(string_terms_vector != "")]
if (intercept_bool == FALSE){
base_formula = paste(base_formula, " - 1 + ")
}
added_terms = paste(string_terms_vector, collapse = " + ")
return(as.formula(paste(base_formula, added_terms)))
}
# creazione delle formule
# per evitare errori dovuti a formule troppo lunghe
options(expressions = 50000)
formula_yes_interaction_yes_intercept <- MakeFormula(c(no_interaction_string,
num_vars_square_string,
qual_qual_interactions_string,
qual_num_interactions_string))
formula_yes_interaction_no_intercept <- MakeFormula(c(no_interaction_string,
num_vars_square_string,
qual_qual_interactions_string,
qual_num_interactions_string),
intercept_bool = FALSE)
formula_yes_interaction_yes_intercept
formula_yes_interaction_no_intercept
# formula senza interazioni
formula_no_interaction_yes_intercept = MakeFormula(no_interaction_string)
formula_no_interaction_no_intercept = MakeFormula(no_interaction_string, intercept_bool = FALSE)
formula_no_interaction_yes_intercept
formula_no_interaction_no_intercept
# l'unica funzione di perdita è il tasso di errata classificazione
source("lift_roc.R")
library(dplyr)
#////////////////////////////////////////////////////////////////////////////
# Costruzione metrica di valutazione e relativo dataframe -------------------
#////////////////////////////////////////////////////////////////////////////
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Qualitativa -------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
MissErr = function(previsti, osservati){
return( 1- sum(previsti == osservati) / length(previsti))
}
# funzione di convenienza
Null.Loss = function(y.pred, y.test, weights = 1){
NULL
}
# °°°°°°°°°°°°°°°°°°°°°°° Warning: °°°°°°°°°°°°°°°°°°°°°°°°°°°°°°
# cambia la funzione di errore per il problema specifico
USED.Loss = function(y.pred, y.test, weights = 1){
return(MissErr(y.pred, y.test))
}
# anche qua
df_err_qual = data.frame(name = NA,
misclassification = NA)
# Funzione per aggiornare il data.frame degli errori
# (inefficiente, ma amen, tanto le operazioni che deve eseguire sono sempre limitate)
# Add the error to the df_error data.frame:
# if the df_error already has a model name in name column with the same as input: update the error value
# otherwise append the new name and error
# arguments:
# @df_error (data.frame): data.frame with columns: [1]: name and [2]: error
# @model_name (char): character with the model name
# @loss_value (num): numeric with the error on the test set
# @return: df_error
Add_Test_Error = function(df_error, model_name, loss_value){
# check if the model name is already in the data.frame
is_name = model_name %in% df_error[,1]
# if yes: get the index and subscribe
if(is_name){
df_error[which(df_error[,1] == model_name),2:ncol(df_error)] = loss_value
}
else{
# get the last index
df_error[NROW(df_error) + 1,] = c(model_name, loss_value)
}
return(df_error)
}
# /////////////////////////////////////////////////////////////////
#------------------------ Stima e Verifica ------------------------
# /////////////////////////////////////////////////////////////////
# Eventualmente modificare la proporzione
id_stima = sample(1:NROW(dati), 0.75 * NROW(dati))
sss = dati[id_stima,]
vvv = dati[-id_stima,]
# In caso di convalida nell'insieme di stima
id_cb1 = sample(1:NROW(sss), 0.8 * NROW(sss))
id_cb2 = setdiff(1:NROW(sss), id_cb1)
# rimozione dataset originale
rm(dati)
# /////////////////////////////////////////////////////////////////
#------------------------ Analisi esplorative ---------------------
# /////////////////////////////////////////////////////////////////
# Analisi esplorativa sulla stima
# eventuali inflazioni di zeri
# valutiamo se è sbilanciata
table(sss$y)
y_rel_freqs = table(sss$y) / NROW(sss)
y_uniques = unique(sss$y)
sorted_y_values = sort(unique(sss$y))
# se per motivi computazionali l'albero sopra non può essere stimato
# aumento il numero di elementi in ogni foglia (sub-ottimale,
# ma meglio di non stimare il modello).
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
mincut = 40))
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Albero -------------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
library(tree)
# se per motivi computazionali l'albero sopra non può essere stimato
# aumento il numero di elementi in ogni foglia (sub-ottimale,
# ma meglio di non stimare il modello).
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
mincut = 40))
# controllo che sia sovraadattato
plot(tree_full)
# potatura
tree_pruned = prune.tree(tree_full, newdata = sss[-id_cb1,])
plot(tree_pruned)
plot(tree_pruned, xlim = c(10, 90))
tree_best_size = tree_pruned$size[which.min(tree_pruned$dev)]
tree_best_size
abline(v = tree_best_size, col = "red")
final_tree_pruned = prune.tree(tree_full, best = tree_best_size)
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
pred_tree_pruned = predict(final_tree_pruned, newdata = vvv, type = "class")
df_err_qual = Add_Test_Error(df_err_qual,
"tree_pruned best",
USED.Loss(pred_tree_pruned, vvv$y))
df_err_qual
rm(tree_full)
gc()
library(snowfall)
# Implementazione in parallelo
library(ranger)
sfInit(cpus = 4, parallel = T)
sfLibrary(ranger)
sfExport(list = c("sss"))
# esportiamo tutti gli oggetti necessari
# scelta del numero di esplicative a ogni split
# adattiamo 50 alberi per core e di ciascun albero ritorniamo l'errore out of bag
# successivamente sommiamo gli errori out of bag per ogni mtry
# massimo numero di esplicative presenti
m_max = NCOL(sss) - 1 # sottraggo 1 per la variabile risposta
# se m_max è grande eventualmente ridurlo per considerazioni computazionali
# regolazione
# procedura sub-ottimale, ma la impiego per ragioni computazionali
# prima scelgo il numero di esplicative a ogni split,
# una volta scelto controllo la convergenza dell'errore basata sul numero di alberi
err = rep(NA, m_max - 1)
# °°°°°°°°°°°°°°°°°°°°°°°°°°°Warning: lento°°°°°°°°°°°°°°°°°°°°°°°°°°°°
for(i in seq(2, m_max)){
sfExport(list = c("i"))
err[i] = mean(sfSapply(rep(1:4),
function(x) ranger(factor(y) ~., data = sss,
mtry = i,
num.trees = 50,
oob.error = TRUE)$prediction.error))
print(i)
gc()
}
err
best_mtry = which.min(err)
best_mtry
# 2
sfExport(list = c("best_mtry"))
# uso il valore trovato e controllo la convergenza rispetto al numero di alberi
err_rf_trees = rep(NA, 90)
# °°°°°°°°°°°°°°°°°°°°°°°°°°°Warning: lento°°°°°°°°°°°°°°°°°°°°°°°°°°°°
for(j in 10:100){
sfExport(list = c("j"))
err_rf_trees[j] = sum(sfSapply(rep(1:4),
function(x) ranger(factor(y) ~., sss,
mtry = best_mtry,
num.trees = j,
oob.error = TRUE)$prediction.error))
print(j)
gc()
}
sfStop()
plot((1:length(err_rf_trees)) * 4, err_rf_trees,
xlab = "numero di alberi bootstrap",
ylab = "errore out of bag",
pch = 16,
main = "Random Forest")
# best_mtry = 2
# modello finale e previsioni
random_forest_model = ranger(factor(y) ~., sss,
mtry = best_mtry,
num.trees = 400,
oob.error = TRUE,
importance = "permutation",
probability = T)
pred_random_forest = predict(random_forest_model, data = vvv, type = "class")
pred_random_forest = predict(random_forest_model, data = vvv, type = "response")
pred_random_forest
df_err_qual = Add_Test_Error(df_err_qual,
"Random Forest",
USED.Loss(pred_random_forest,vvv$y))
df_err_qual
pred_random_forest$predictions
dim(pred_random_forest)
pred_random_forest = predict(random_forest_model, data = vvv, type = "response")$predictions
dim(pred_random_forest)
apply(pred_random_forest, 1, which.max)
pred_random_forest_class = apply(pred_random_forest, 1, which.max)
pred_random_forest_class = sorted_y_values[pred_random_forest_class]
df_err_qual = Add_Test_Error(df_err_qual,
"Random Forest",
USED.Loss(pred_random_forest_class,vvv$y))
df_err_qual
# Importanza delle variabili
vimp = importance(random_forest_model)
dotchart(vimp[order(vimp)])
rm(random_forest_model)
gc()
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# MARS ---------------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
library(earth)
# attenzione ale interazioni: degree = 2
# interazioni fino al secondo ordine
m_mars = earth(y ~., sss, degree = 2)
# attenzione ale interazioni: degree = 2
# interazioni fino al secondo ordine
m_mars = earth(Y_sss ~., sss, degree = 2)
#carico la libreria nnet in cui c'e' il comando class.ind che crea le
# variabili indicatrici per le modalità della risposta
library(nnet)
Y_sss = class.ind(sss$y)
# attenzione ale interazioni: degree = 2
# interazioni fino al secondo ordine
m_mars = earth(Y_sss ~., sss, degree = 2)
# per ogni numero di basi incluse
m_mars$gcv.per.subset
plot(m_mars$gcv.per.subset, pch = 16,
xlab = "Numero di basi",
ylab = "GCV")
abline(v = which.min(m_mars$gcv.per.subset), col = "gold",
main = "MARS")
m_mars
# attenzione ale interazioni: degree = 2
# interazioni fino al secondo ordine
m_mars = earth(Y_sss ~., sss[,-y_index], degree = 2)
# per ogni numero di basi incluse
m_mars$gcv.per.subset
plot(m_mars$gcv.per.subset, pch = 16,
xlab = "Numero di basi",
ylab = "GCV")
abline(v = which.min(m_mars$gcv.per.subset), col = "gold",
main = "MARS")
summary(m_mars)
pred_mars = apply(predict(m_mars, vvv), 1, which.max)
pred_mars_class = sorted_y_values[pred_mars]
df_err_qual = Add_Test_Error(df_err_qual,
"mars",
USED.Loss(pred_mars_class, vvv$y))
df_err_qual
rm(tree_full)
gc()
rm(m_mars)
gc()
