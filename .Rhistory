# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
# Implementazione in parallelo
library(ranger)
library(snowfall)
sfInit(cpus = N_CORES, parallel = T)
sfLibrary(ranger)
sfExport(list = c("sss"))
# massimo numero di esplicative presenti
m_max = NCOL(sss) - 1 # sottraggo 1 per la variabile risposta
# se m_max è grande eventualmente ridurlo per considerazioni computazionali
RF_MAX_VARIABLES = m_max
RF_N_BS_TREES = 200 #up to 400
test = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
test = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
#' @description this function can be used (separately, not simultaneuosly) for two parameters check
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the vector my_n_variables (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector my_n_bs_trees (supposed to be a sequence)
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvRFParallel = function(my_id_list_cv,
my_metric_names,
my_data,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS,
my_ncores = N_CORES,
use_only_first_fold = FALSE,
is_classification = FALSE,
my_threshold = 0.5){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
if(use_only_first_fold == TRUE){
n_k_fold = 1
}
# init parallel
sfInit(cpus = my_ncores, parallel = T)
sfLibrary(randomForest)
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
# allocate relevant variables in cluster
sfExport(list = c("my_data", my_metrics_functions,
"my_n_bs_trees", "tuning_parameter_length",
"my_n_variables", "my_weights",
"my_threshold", "is_classification"))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
PredictFunction = function(my_pred_mtry, my_pred_n_trees){
if(is_classification == TRUE){
model = randomForest(factor(y) ~.,
data = my_data[id_train,],
mtry = my_pred_mtry,
ntree = my_pred_n_trees)
temp_predictions = predict(model,
type = "prob",
newdata = my_data[id_test,])[,2] > my_threshold
}
if(is_classification == FALSE){
model = randomForest(y ~.,
data = my_data[id_train,],
mtry = my_pred_mtry,
ntree = my_pred_n_trees)
temp_predictions = predict(model,
newdata = my_data[id_test,])
}
return(temp_predictions)
}
sfExport(list = c("id_train", "id_test", "PredictFunction"))
if(fix_trees_bool == TRUE){
# for better readability
temp_metric = sfLapply(1:tuning_parameter_length,
fun = function(m)
USED.Metrics(PredictFunction(my_pred_mtry = m,
my_pred_n_trees = my_n_bs_trees),
my_data$y[id_test],
weights = my_weights[id_test]))
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,my_n_variables,] = matrix(unlist(temp_metric), ncol = my_n_metrics, byrow = T)
rm(temp_metric)
gc()
}
else{
# for better readability
temp_metric = sfLapply(1:tuning_parameter_length,
fun = function(t)
USED.Metrics(PredictFunction(my_pred_mtry = my_n_variables,
my_pred_n_trees = t),
my_data$y[id_test],
weights = my_weights))
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,1:tuning_parameter_length,] = matrix(unlist(temp_metric), ncol = my_n_metrics, byrow = T)
rm(temp_metric)
gc()
}
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
test = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 1:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
test
# se m_max è grande eventualmente ridurlo per considerazioni computazionali
RF_MAX_VARIABLES = m_max
RF_N_BS_TREES = 400 #up to 400
rf_cv_metrics = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 2:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
RF_N_BS_TREES = 300 #up to 400
rf_cv_metrics = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 2:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
#' @description this function can be used (separately, not simultaneuosly) for two parameters check
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the vector my_n_variables (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector my_n_bs_trees (supposed to be a sequence)
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvRFParallel = function(my_id_list_cv,
my_metric_names,
my_data,
my_n_variables = 2:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS,
my_ncores = N_CORES,
use_only_first_fold = FALSE,
is_classification = FALSE,
my_threshold = 0.5){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
if(use_only_first_fold == TRUE){
n_k_fold = 1
}
# init parallel
sfInit(cpus = my_ncores, parallel = T)
sfLibrary(randomForest)
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, tuning_parameter_length, my_n_metrics))
# allocate relevant variables in cluster
sfExport(list = c("my_data", my_metrics_functions,
"my_n_bs_trees", "tuning_parameter_length",
"my_n_variables", "my_weights",
"my_threshold", "is_classification"))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
PredictFunction = function(my_pred_mtry, my_pred_n_trees){
if(is_classification == TRUE){
model = randomForest(factor(y) ~.,
data = my_data[id_train,],
mtry = my_pred_mtry,
ntree = my_pred_n_trees)
temp_predictions = predict(model,
type = "prob",
newdata = my_data[id_test,])[,2] > my_threshold
}
if(is_classification == FALSE){
model = randomForest(y ~.,
data = my_data[id_train,],
mtry = my_pred_mtry,
ntree = my_pred_n_trees)
temp_predictions = predict(model,
newdata = my_data[id_test,])
}
return(temp_predictions)
}
sfExport(list = c("id_train", "id_test", "PredictFunction"))
if(fix_trees_bool == TRUE){
# for better readability
temp_metric = sfLapply(1:tuning_parameter_length,
fun = function(m)
USED.Metrics(PredictFunction(my_pred_mtry = m,
my_pred_n_trees = my_n_bs_trees),
my_data$y[id_test],
weights = my_weights[id_test]))
print(temp_metric)
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,my_n_variables,] = matrix(unlist(temp_metric),
ncol = my_n_metrics,
byrow = T)
rm(temp_metric)
gc()
}
else{
# for better readability
temp_metric = sfLapply(1:tuning_parameter_length,
fun = function(t)
USED.Metrics(PredictFunction(my_pred_mtry = my_n_variables,
my_pred_n_trees = t),
my_data$y[id_test],
weights = my_weights))
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,tuning_parameter_length,] = matrix(unlist(temp_metric),
ncol = my_n_metrics,
byrow = T)
rm(temp_metric)
gc()
}
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
# potential problems: too heavy vector allocated
rf_cv_metrics = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 2:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
#' @description this function can be used (separately, not simultaneuosly) for two parameters check
#' 1) if fix_tress_bool == TRUE -> my_n_bs_trees is fixed at its maximum if not already an integer
#' and the procedure compute the CV error for varying number of variables at each split
#' according to the vector my_n_variables (supposed to be a sequence)
#' 2) if fix_tress_bool == FALSE -> my_n_variables is fixed at its maximum if not already an integer,
#' but a warning is given, because it should be just an integer, not a vector.
#' the procedure compute the CV error for varying number of bootstrap trees
#' according to the the vector my_n_bs_trees (supposed to be a sequence)
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvRFParallel = function(my_id_list_cv,
my_metric_names,
my_data,
my_n_variables = 2:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS,
my_ncores = N_CORES,
use_only_first_fold = FALSE,
is_classification = FALSE,
my_threshold = 0.5){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
if(use_only_first_fold == TRUE){
n_k_fold = 1
}
# init parallel
sfInit(cpus = my_ncores, parallel = T)
sfLibrary(randomForest)
# fixed number of bootstrap trees, number of variables changes
if(fix_trees_bool == TRUE){
tuning_parameter_length = length(my_n_variables)
# fix the number of trees to max if my_n_bs_trees is not already an int
my_n_bs_trees = max(my_n_bs_trees)
}
# fixed number of number of variables, number of bootstrap tress changes
else{
tuning_parameter_length = length(my_n_bs_trees)
# check warning
if(length(my_n_variables) > 1){
print("Warning: my_n_variables should be an integer, not a sequence of numbers, maximum is taken")
}
my_n_variables = max(my_n_variables)
}
temp_metrics_array_cv = array(NA, dim = c(n_k_fold,
tuning_parameter_length,
my_n_metrics))
# allocate relevant variables in cluster
sfExport(list = c("my_data", my_metrics_functions,
"my_n_bs_trees", "tuning_parameter_length",
"my_n_variables", "my_weights",
"my_threshold", "is_classification"))
for (k in 1:n_k_fold){
id_train = unlist(my_id_list_cv[-k])
id_test = my_id_list_cv[[k]]
# it's ugly I know and a bad pratice but I'll do two separate loop based on if condition
PredictFunction = function(my_pred_mtry, my_pred_n_trees){
if(is_classification == TRUE){
model = randomForest(factor(y) ~.,
data = my_data[id_train,],
mtry = my_pred_mtry,
ntree = my_pred_n_trees)
temp_predictions = predict(model,
type = "prob",
newdata = my_data[id_test,])[,2] > my_threshold
}
if(is_classification == FALSE){
model = randomForest(y ~.,
data = my_data[id_train,],
mtry = my_pred_mtry,
ntree = my_pred_n_trees)
temp_predictions = predict(model,
newdata = my_data[id_test,])
}
return(temp_predictions)
}
sfExport(list = c("id_train", "id_test", "PredictFunction"))
if(fix_trees_bool == TRUE){
# for better readability
temp_metric = sfLapply(1:tuning_parameter_length,
fun = function(m)
USED.Metrics(PredictFunction(my_pred_mtry = m,
my_pred_n_trees = my_n_bs_trees),
my_data$y[id_test],
weights = my_weights[id_test]))
print(temp_metric)
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,tuning_parameter_length,] = matrix(unlist(temp_metric),
ncol = my_n_metrics,
byrow = T)
rm(temp_metric)
gc()
}
else{
# for better readability
temp_metric = sfLapply(1:tuning_parameter_length,
fun = function(t)
USED.Metrics(PredictFunction(my_pred_mtry = my_n_variables,
my_pred_n_trees = t),
my_data$y[id_test],
weights = my_weights))
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,tuning_parameter_length,] = matrix(unlist(temp_metric),
ncol = my_n_metrics,
byrow = T)
rm(temp_metric)
gc()
}
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = tuning_parameter_length, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
# potential problems: too heavy vector allocated
rf_cv_metrics = ManualCvRFParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_n_variables = 2:RF_MAX_VARIABLES,
my_n_bs_trees = 10:RF_N_BS_TREES,
fix_trees_bool = TRUE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
library(ipred)
?bagging()
library(ipred)
sfInit(cpus = N_CORES, parallel = T)
sfExport(list = c("sss"))
sfLibrary(ipred)
err_bg_trees = rep(NA, 90)
# controllo la convergenza dell'errore rispetto al numero di alberi
# parto da 40 alberi bootstrap
for(j in 10:100){
sfExport(list = c("j"))
err_bg_trees[j] = sum(sfSapply(rep(1:4),
function(x) bagging(factor(y) ~., sss,
nbag = j,
coob = TRUE)$err))
print(j*4)
gc()
}
