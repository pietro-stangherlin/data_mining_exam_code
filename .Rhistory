}
# creazione delle formule
# per evitare errori dovuti a formule troppo lunghe
options(expressions = 50000)
formula_yes_interaction_yes_intercept <- MakeFormula(c(no_interaction_string,
num_vars_square_string,
qual_qual_interactions_string,
qual_num_interactions_string))
formula_yes_interaction_no_intercept <- MakeFormula(c(no_interaction_string,
num_vars_square_string,
qual_qual_interactions_string,
qual_num_interactions_string),
intercept_bool = FALSE)
formula_yes_interaction_yes_intercept
formula_yes_interaction_no_intercept
# formula senza interazioni
formula_no_interaction_yes_intercept = MakeFormula(no_interaction_string)
formula_no_interaction_no_intercept = MakeFormula(no_interaction_string, intercept_bool = FALSE)
formula_no_interaction_yes_intercept
formula_no_interaction_no_intercept
# /////////////////////////////////////////
# Backup data.frame + environment ---------
# ////////////////////////////////////////
save(dati,
y_index,
var_qual_index, var_qual_names,
var_num_index, var_num_names,
formula_no_interaction_no_intercept,
formula_no_interaction_yes_intercept,
formula_yes_interaction_no_intercept,
formula_yes_interaction_yes_intercept,
FIGURES_FOLDER_RELATIVE_PATH,
MODELS_FOLDER_RELATIVE_PATH,
file = "result_preprocessing.Rdata")
# if necessary delete all
# rm(list = ls())
# in case of problems: load only useful objects
# load("result_preprocessing.Rdata")
# ///////////////////////////////////
# Save output on file ---------------
# //////////////////////////////////
# text.txt -------------
# # close previoulsy opened sink (if opened) -> I should make a control
# sink()
# initialize the output .txt file to regularly write on in case
# the software crashes
# open new sink
TEXT_OUTPUT_FILE_NAME = "text_output_models.txt"
# open sink
sink(TEXT_OUTPUT_FILE_NAME, append = TRUE, split = TRUE)
# data wrangling
library(dplyr)
# parallel computing
library(snowfall)
# number of cores
N_CORES = parallel::detectCores()
#////////////////////////////////////////////////////////////////////////////
# Metrics and data.frame --------------------------------------------------
#////////////////////////////////////////////////////////////////////////////
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Quantitative response ---------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
source("loss_functions.R")
# °°°°°°°°°°°°°°°°°°°°°°° Warning: °°°°°°°°°°°°°°°°°°°°°°°°°°°°°°
# change functions for specific problems
# in generale uso sia MAE che MSE
USED.Metrics = function(y.pred, y.test, weights = 1){
return(c(MAE.Loss(y.pred, y.test, weights), MSE.Loss(y.pred, y.test, weights)))
}
df_metrics = data.frame(name = NA, MAE = NA, MSE = NA)
METRICS_NAMES = colnames(df_metrics[,-1])
N_METRICS = length(METRICS_NAMES)
# names used to extract the metric added to df_metrics
# change based on the spefific problem
METRIC_VALUES_NAME = "metric_values"
METRIC_CHOSEN_NAME = "MSE"
# names used for accessing list CV matrix (actual metrics and metrics se)
LIST_METRICS_ACCESS_NAME = "metrics"
LIST_SD_ACCESS_NAME = "se"
# metrics names + USED.Loss
# WARNING: the order should be same as in df_metrics
MY_USED_METRICS = c("USED.Metrics", "MAE.Loss", "MSE.Loss")
# /////////////////////////////////////////////////////////////////
#------------------------ Train & Test ------------------------
# /////////////////////////////////////////////////////////////////
# eventually change the proportion
id_stima = sample(1:NROW(dati), 0.75 * NROW(dati))
sss = dati[id_stima,]
vvv = dati[-id_stima,]
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Parameter tuning: Train & Test on Train subset  --------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
id_cb1 = sample(1:NROW(sss), 0.8 * NROW(sss))
# delete original data.frame from main memory
rm(dati)
gc()
# ///////////////////////////////////
# Weights ---------------
# //////////////////////////////////
# weights used for each metric function
# default 1
MY_WEIGHTS_sss = rep(1, NROW(sss))
MY_WEIGHTS_vvv = rep(1, NROW(vvv))
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Parameter tuning: cross validation on train: building cv folds  -------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
K_FOLDS = 10
NROW_sss = NROW(sss)
SHUFFLED_ID = sample(1:NROW_sss, NROW_sss)
# NOTE: if the row number of sss is not a multiple of K_FOLDS
# the last fold repeats some ids from the first
# this is fixed in the code below
id_matrix_cv = matrix(SHUFFLED_ID, ncol = K_FOLDS)
# conversion of matrix in list of elements: each element contains a subset of ids
ID_CV_LIST = list()
for(j in 1:ncol(id_matrix_cv)){
ID_CV_LIST[[j]] = id_matrix_cv[,j]
}
rm(id_matrix_cv)
gc()
# repeated ids fixing
integer_division_cv = NROW_sss %/% K_FOLDS
modulo_cv = NROW_sss %% K_FOLDS
if(modulo_cv != 0){
ID_CV_LIST[[K_FOLDS]] = ID_CV_LIST[[K_FOLDS]][1:integer_division_cv]
}
source("cv_functions.R")
# FALSE = traditional CV on all folds
# TRUE -> use only first fold to test and all other to fit
USE_ONLY_FIRST_FOLD = FALSE
# /////////////////////////////////////////////////////////////////
#------------------------ Explorative Data Analysis ---------------
# /////////////////////////////////////////////////////////////////
# (on train set)
# check distribution of marginal response
hist(sss$y,nclass = 100)
hist(sss$y,nclass = 100)
summary(sss$y)
# check logaritm, a transformation (traslation) is maybe needed before
hist(log(sss$y), nclass = 100)
df_metrics = Add_Test_Metric(df_metrics,
"sss mean",
USED.Metrics(mean(sss$y),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics = Add_Test_Metric(df_metrics,
"sss median",
USED.Metrics(median(sss$y),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics = na.omit(df_metrics)
df_metrics
# sparse is preferred is there are many categorical predictors (sparse matrix)
library(Matrix)
X_mm_no_interaction_sss =  sparse.model.matrix(formula_no_interaction_no_intercept, data = sss)
X_mm_no_interaction_vvv =  sparse.model.matrix(formula_no_interaction_no_intercept, data = vvv)
# computational heavy
X_mm_yes_interaction_sss =  sparse.model.matrix(formula_yes_interaction_no_intercept, data = sss)
X_mm_yes_interaction_vvv =  sparse.model.matrix(formula_yes_interaction_no_intercept, data = vvv)
library(glmnet)
# criterion to choose the model: "1se" or "lmin"
cv_criterion = "lambda.1se"
lambda_vals = glmnet(x = X_mm_no_interaction_sss, y = sss$y,
alpha = 0, lambda.min.ratio = 1e-07)$lambda
ridge_no_interaction_metrics = ManualCvGlmnet(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_x = X_mm_no_interaction_sss,
my_y = sss$y,
my_alpha = 0,
my_lambda_vals = lambda_vals,
my_weights = MY_WEIGHTS_sss,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
ridge_no_int_best_summary = CvMetricBest(my_param_values = lambda_vals,
my_metric_matrix = ridge_no_interaction_metrics[["metrics"]],
my_one_se_best = TRUE,
my_higher_more_complex = FALSE,
my_se_matrix = ridge_no_interaction_metrics[["se"]],
my_metric_names = METRICS_NAMES)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = log(lambda_vals),
my_metric_matrix = ridge_no_interaction_metrics[["metrics"]],
my_se_matrix = ridge_no_interaction_metrics[["se"]],
my_best_param_values =log(ExtractBestParams(ridge_no_int_best_summary)),
my_metric_names = METRICS_NAMES,
my_main = "Ridge no interaction CV metrics",
my_xlab = " log lambda")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"ridge_no_int_metrics_plot.jpeg",
collapse = ""))
print("ridge_no_int_best_summary")
ridge_no_int_best_summary
ridge_no_interaction = glmnet(x = X_mm_no_interaction_sss,
y = sss$y,
alpha = 0,
lambda = ridge_no_int_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]])
df_metrics = Add_Test_Metric(df_metrics,
"ridge_no_interaction",
USED.Metrics(predict(ridge_no_interaction, newx = X_mm_no_interaction_vvv),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
file_name_ridge_no_interaction = paste(MODELS_FOLDER_RELATIVE_PATH,
"ridge_no_interaction",
".Rdata", collapse = "", sep = "")
save(ridge_no_interaction, file = file_name_ridge_no_interaction)
rm(ridge_no_interaction)
gc()
# YES Interaction -----------
lambda_vals = glmnet(x = X_mm_yes_interaction_sss, y = sss$y,
alpha = 0, lambda.min.ratio = 1e-07)$lambda
ridge_yes_interaction_metrics = ManualCvGlmnetParallel(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_x = X_mm_yes_interaction_sss,
my_y = sss$y,
my_alpha = 0,
my_lambda_vals = lambda_vals,
my_weights = MY_WEIGHTS_sss,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES,
use_only_first_fold = TRUE)
ridge_yes_int_best_summary = CvMetricBest(my_param_values = lambda_vals,
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_one_se_best = FALSE,
my_higher_more_complex = FALSE,
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_metric_names = METRICS_NAMES)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = log(lambda_vals),
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_best_param_values =log(ExtractBestParams(ridge_yes_int_best_summary)),
my_metric_names = METRICS_NAMES,
my_main = "Ridge yes interaction metrics",
my_xlab = " log lambda")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"ridge_yes_int_metrics_plot.jpeg",
collapse = ""))
lambda_vals = glmnet(x = X_mm_no_interaction_sss, y = sss$y,
alpha = 1, lambda.min.ratio = 1e-07)$lambda
lasso_no_interaction_metrics = ManualCvGlmnet(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_x = X_mm_no_interaction_sss,
my_y = sss$y,
my_alpha = 1,
my_lambda_vals = lambda_vals,
my_weights = MY_WEIGHTS_sss,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
lasso_no_int_best_summary = CvMetricBest(my_param_values = lambda_vals,
my_metric_matrix = lasso_no_interaction_metrics[["metrics"]],
my_one_se_best = TRUE,
my_higher_more_complex = FALSE,
my_se_matrix = lasso_no_interaction_metrics[["se"]],
my_metric_names = METRICS_NAMES)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = log(lambda_vals),
my_metric_matrix = lasso_no_interaction_metrics[["metrics"]],
my_se_matrix = lasso_no_interaction_metrics[["se"]],
my_best_param_values =log(ExtractBestParams(lasso_no_int_best_summary)),
my_metric_names = METRICS_NAMES,
my_main = "lasso no interaction CV metrics",
my_xlab = " log lambda")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"lasso_no_int_metrics_plot.jpeg",
collapse = ""))
print("lasso_no_int_best_summary")
lasso_no_int_best_summary
lasso_no_interaction = glmnet(x = X_mm_no_interaction_sss,
y = sss$y,
alpha = 1,
lambda = lasso_no_int_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]])
# default: overfit
tree_full = tree(y ~.,
data = sss[id_cb1,],
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
minsize = 5))
# check overfitting
plot(tree_full)
TREE_MAX_SIZE = 100
# if parallel shows problems use the non parallel version
tree_cv_metrics = ManualCvTreeParallel(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-04,
my_minsize = 5,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
tree_cv_metrics = ManualCvTree(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-04,
my_minsize = 5,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
tree_cv_metrics = ManualCvTree(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-04,
my_minsize = 10,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
tree_cv_metrics = ManualCvTree(my_id_list_cv_train = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-03,
my_minsize = 10,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
tree_best_summary = CvMetricBest(my_param_values = 2:TREE_MAX_SIZE,
my_metric_matrix = tree_cv_metrics[["metrics"]],
my_one_se_best = TRUE,
my_higher_more_complex = TRUE,
my_se_matrix = tree_cv_metrics[["se"]],
my_metric_names = METRICS_NAMES)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = 2:TREE_MAX_SIZE,
my_metric_matrix = tree_cv_metrics[["metrics"]],
my_se_matrix = tree_cv_metrics[["se"]],
my_best_param_values = ExtractBestParams(tree_best_summary),
my_metric_names = METRICS_NAMES,
my_main = "Tree CV metrics",
my_xlab = "size")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"tree_cv_metrics_plot.jpeg",
collapse = ""))
tree_best_size = tree_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]]
print("tree best size")
tree_best_size
final_tree_pruned = prune.tree(tree_full,
best = tree_best_size)
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)
df_metrics = Add_Test_Metric(df_metrics,
"tree_pruned best",
USED.Metrics(predict(final_tree_pruned, newdata = vvv),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
file_name_final_tree_pruned = paste(MODELS_FOLDER_RELATIVE_PATH,
"final_tree_pruned",
".Rdata", collapse = "", sep = "")
save(final_tree_pruned, file = file_name_final_tree_pruned)
rm(final_tree_pruned)
rm(tree_full)
gc()
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
temp_plot_function = function(){
plot(final_tree_pruned)
text(final_tree_pruned, cex = 0.7)}
PlotAndSave(temp_plot_function,
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"tree_pruned_plot.jpeg",
collapse = ""))
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Modello Additivo ---------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
library(gam)
# stepwise forward: AIC based on generalized df
gam0 = gam(y ~ 1, data = sss)
# gam recognizes factor predictors
my_gam_scope = gam.scope(sss[,-y_index], arg = c("df=2", "df=3", "df=4", "df=5", "df=6"))
gam_step = step.Gam(gam0, scope = my_gam_scope)
df_metrics = Add_Test_Metric(df_metrics,
"gam_step",
USED.Metrics(predict(gam_step, newdata = vvv),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
file_name_gam_step = paste(MODELS_FOLDER_RELATIVE_PATH,
"gam_step",
".Rdata", collapse = "", sep = "")
save(gam_step, file = file_name_gam_step)
rm(gam_step)
rm(gam0)
gc()
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
# data.frame case
num_index = which(colnames(sss[,-y_index]) %in% var_num_names)
factor_index = setdiff(1:NCOL(sss[,-y_index]), num_index)
library(polspline)
mars_step = polymars(responses = sss$y,
predictors = sss[,-y_index],
gcv = 1,
factors = factor_index,
maxsize = 50)
print("mars min size gcv")
min_size_mars = mars_step$fitting$size[which.min(mars_step$fitting$GCV)]
min_size_mars
temp_plot_function = function(){
plot(mars_step$fitting$size, mars_step$fitting$GCV,
col = as.factor(mars_step$fitting$`0/1`),
pch = 16,
xlab = "basis number",
ylab = "GCV",
main = "MARS step GCV")
legend(c("topright"),
legend = c("growing", "pruning"),
col = c("black","red"),
pch = 16)
abline(v = min_size_mars)
}
PlotAndSave(temp_plot_function, my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"mars_gcv_plot.jpeg",
collapse = ""))
df_metrics = Add_Test_Metric(df_metrics,
"MARS",
USED.Metrics(predict(mars_step, x = vvv),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
mars_names = colnames(sss[,-y_index])
file_name_mars_step = paste(MODELS_FOLDER_RELATIVE_PATH,
"mars_step",
".Rdata", collapse = "", sep = "")
save(mars_step,
mars_names,
file = file_name_mars_step)
rm(mars_step)
gc()
# max number of ridge functions
PPR_MAX_RIDGE_FUNCTIONS = 4
# possible spline degrees of freedom
PPR_DF_SM = 2:6
ppr_metrics = PPRRegulationCVParallel(my_data = sss,
my_id_list_cv_train = ID_CV_LIST,
my_max_ridge_functions = PPR_MAX_RIDGE_FUNCTIONS,
my_spline_df = PPR_DF_SM,
my_metrics_names = METRICS_NAMES,
my_weights = MY_WEIGHTS_sss,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES,
use_only_first_fold = TRUE)
ppr_best_params = PPRExtractBestParams(ppr_metrics)
print("ppr best params")
ppr_best_params
ppr_model = ppr(y ~ .,
data = sss,
nterms = ppr_best_params[[METRIC_CHOSEN_NAME]][["n_ridge_functions"]],
sm.method = "spline",
df = ppr_best_params[[METRIC_CHOSEN_NAME]][["spline_df"]])
df_metrics = Add_Test_Metric(df_metrics,
"PPR",
USED.Metrics(predict(ppr_model, vvv),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
file_name_ppr_model = paste(MODELS_FOLDER_RELATIVE_PATH,
"ppr_model",
".Rdata", collapse = "", sep = "")
save(ppr_model, file = file_name_ppr_model)
rm(ppr_model)
gc()
# Implementazione in parallelo
library(ranger)
# massimo numero di esplicative presenti
m_max = NCOL(sss) - 2 # sottraggo 1 per la variabile risposta
# regolazione
# procedura sub-ottimale, ma la impiego per ragioni computazionali
# prima scelgo il numero di esplicative a ogni split,
# una volta scelto controllo la convergenza dell'errore basata sul numero di alberi
err = rep(NA, m_max)
for(i in seq(2, m_max)){
sfExport(list = c("i"))
err[i] = ranger(y ~., data = sss,
mtry = i,
num.trees = RF_ITER,
probability = TRUE,
oob.error = TRUE)$prediction.error
print(paste("mtry: ", i, collapse = ""))
gc()
}
# °°°°°°°°°°°°°°°°°°°°°°°°°°°Warning: lento°°°°°°°°°°°°°°°°°°°°°°°°°°°°
RF_TREE_NUMBER_SEQ = seq(10, 400, 10)
err_rf_trees = rep(NA, length(RF_TREE_NUMBER_SEQ))
# °°°°°°°°°°°°°°°°°°°°°°°°°°°Warning: lento°°°°°°°°°°°°°°°°°°°°°°°°°°°°
for(j in 1:length(RF_TREE_NUMBER_SEQ)){
err_rf_trees[j] = ranger(y ~., data = sss,
mtry = best_mtry,
num.trees = RF_TREE_NUMBER_SEQ[j],
oob.error = TRUE)$prediction.error
print(paste("number of trees: ", RF_TREE_NUMBER_SEQ[j], collapse = ""))
}
# regolazione
# procedura sub-ottimale, ma la impiego per ragioni computazionali
# prima scelgo il numero di esplicative a ogni split,
# una volta scelto controllo la convergenza dell'errore basata sul numero di alberi
err = rep(NA, m_max)
RF_ITER = 300
for(i in seq(2, m_max)){
sfExport(list = c("i"))
err[i] = ranger(y ~., data = sss,
mtry = i,
num.trees = RF_ITER,
probability = TRUE,
oob.error = TRUE)$prediction.error
print(paste("mtry: ", i, collapse = ""))
gc()
}
