if(is_classification == TRUE){
temp_predictions = as.numeric(temp_predictions > my_threshold)
}
return(USED.Metrics(y.pred = temp_predictions,
y.test = my_y[id_test],
weights = my_weights[id_test]))
}
sfExport(list = c("id_train", "id_test", "ParallelFunction"))
temp_metrics = sfLapply(my_lambda_vals,
fun = ParallelFunction)
# unlist to the right dimensions matrix
temp_metrics_array_cv[k,,] = matrix(unlist(temp_metrics),
nrow = length(lambda_vals),
ncol = my_n_metrics,
byrow = T)
rm(temp_metrics)
gc()
print(paste("fold ", k, collapse = ""))
}
# stop parallel cluster
sfStop()
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = length(my_lambda_vals), ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = length(my_lambda_vals), ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
ridge_yes_interaction_metrics = ManualCvGlmnetParallel(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_x = X_mm_yes_interaction_sss,
my_y = sss$y,
my_alpha = 0,
my_lambda_vals = lambda_vals,
my_weights = MY_WEIGHTS_sss,
my_metrics_functions = MY_USED_METRICS,
my_ncores = N_CORES,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
ridge_yes_interaction_metrics
ridge_yes_int_best_summary = CvMetricBest(my_param_values = lambda_vals,
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_one_se_best = FALSE,
my_higher_more_complex = FALSE,
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_metric_names = METRICS_NAMES,
indexes_metric_max = 4)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = log(lambda_vals),
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_best_param_values =log(ExtractBestParams(ridge_yes_int_best_summary)),
my_metric_names = METRICS_NAMES,
my_main = "Ridge yes interaction metrics",
my_xlab = " log lambda")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"ridge_yes_int_metrics_plot.jpeg",
collapse = ""))
ridge_yes_int_best_summary = CvMetricBest(my_param_values = lambda_vals,
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_one_se_best = TRUE,
my_higher_more_complex = FALSE,
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_metric_names = METRICS_NAMES,
indexes_metric_max = 4)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = log(lambda_vals),
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_best_param_values =log(ExtractBestParams(ridge_yes_int_best_summary)),
my_metric_names = METRICS_NAMES,
my_main = "Ridge yes interaction metrics",
my_xlab = " log lambda")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"ridge_yes_int_metrics_plot.jpeg",
collapse = ""))
print("ridge_yes_int_best_summary")
ridge_yes_int_best_summary
ridge_yes_interaction = glmnet(x = X_mm_yes_interaction_sss,
y = sss$y,
alpha = 0,
lambda = ridge_yes_int_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]])
# previsione ed errore
df_metrics = Add_Test_Metric(df_metrics,
"ridge_yes_interaction",
USED.Metrics(predict(ridge_yes_interaction, newx = X_mm_yes_interaction_vvv),
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
# save the df_metrics as .Rdata
save(df_metrics, file = "df_metrics.Rdata")
file_name_ridge_yes_interaction = paste(MODELS_FOLDER_RELATIVE_PATH,
"ridge_yes_interaction",
".Rdata", collapse = "", sep = "")
save(ridge_yes_interaction, file = file_name_ridge_yes_interaction)
ridge_yes_int_best_summary[[METRIC_CHOSEN_NAME]][["best_param_value"]]
# previsione ed errore
df_metrics = Add_Test_Metric(df_metrics,
"ridge_yes_interaction",
USED.Metrics(predict(ridge_yes_interaction,
newx = X_mm_yes_interaction_vvv) > MY_THRESHOLD,
vvv$y,
weights = MY_WEIGHTS_vvv))
df_metrics
ridge_yes_interaction_metrics = ManualCvGlmnet(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_x = X_mm_yes_interaction_sss,
my_y = sss$y,
my_alpha = 0,
my_lambda_vals = lambda_vals,
my_weights = MY_WEIGHTS_sss,
use_only_first_fold = TRUE,
is_classification = TRUE,
my_threshold = MY_THRESHOLD)
ridge_yes_int_best_summary = CvMetricBest(my_param_values = lambda_vals,
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_one_se_best = TRUE,
my_higher_more_complex = FALSE,
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_metric_names = METRICS_NAMES,
indexes_metric_max = 4)
PlotAndSave(function()(
PlotCvMetrics(my_param_values = log(lambda_vals),
my_metric_matrix = ridge_yes_interaction_metrics[["metrics"]],
my_se_matrix = ridge_yes_interaction_metrics[["se"]],
my_best_param_values =log(ExtractBestParams(ridge_yes_int_best_summary)),
my_metric_names = METRICS_NAMES,
my_main = "Ridge yes interaction metrics",
my_xlab = " log lambda")),
my_path_plot = paste(FIGURES_FOLDER_RELATIVE_PATH,
"ridge_yes_int_metrics_plot.jpeg",
collapse = ""))
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# Tree -------------------------------------
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
library(tree)
?tree
# default: overfit
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
minsize = 5))
# default: overfit
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
minsize = 5))
# check overfitting
plot(tree_full)
predict(tree_full, newdata = vvv)
x = predict(tree_full, newdata = vvv)
dim(x)
colnames(x)
which(colnames(x) == "1")
?tree
?tree.control
# default: overfit
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
split = "gini",
control = tree.control(nobs = length(id_cb1),
mindev = 1e-04,
minsize = 5))
# default: overfit
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
split = "gini",
control = tree.control(nobs = length(id_cb1),
mindev = 1e-03,
minsize = 5))
# default: overfit
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
split = "gini",
control = tree.control(nobs = length(id_cb1),
mindev = 1e-01,
minsize = 5))
# default: overfit
tree_full = tree(factor(y) ~.,
data = sss[id_cb1,],
split = "gini",
control = tree.control(nobs = length(id_cb1),
mindev = 1e-01,
minsize = 20))
head(vvv)
predict(tree_full, type = "class", newdata = vvv) %>% head
max("a", "b")
which.max("a", "b")
which.max(c("a", "b"))
factor(c(0,1,1,0))
head(x)
which(colnames(x) == 1)
#' @param use_only_first_fold (bool): if yes fit the model on all except the first fold
#' and compute the metrics on that
#'
#' @param is_classification (bool): if TRUE adapt the metrics to classification problem
#' using the threshold
#' @param my_threshold (num): classification threshold used
#'
#' @param my_max_size (int): max size of the pruned tree
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvTree = function(n_k_fold,
my_id_list_cv,my_n_metrics,
my_metric_names,
my_data,
my_max_size = TREE_MAX_SIZE,
my_weights = 1,
my_mindev = 1e-05,
my_minsize = 2,
use_only_first_fold = FALSE,
is_classification = FALSE,
my_threshold = 0.5){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
if(use_only_first_fold == TRUE){
n_k_fold = 1
}
# switch to factor response
if(is_classification == TRUE){
my_data$y = factor(my_data$y)
}
# we use my_max_size - 1 because we start with size = 2
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, my_max_size - 1, my_n_metrics))
fold_indexes = 1:n_k_fold
for (k in fold_indexes) {
# k-th fold set is used as validation set
id_test = my_id_list_cv[[k]]
# one set among those remaining is chosen as the pruning set
# specifically we choose the one corresponding to the one with the minimum index
# among those not in the validation set
id_pruning = min(fold_indexes[-k])
# the remaining fold sets are used as training set
id_train = unlist(my_id_list_cv[-c(k, id_pruning)])
# full grown tree
temp_tree_full = tree(y ~.,
data = my_data[id_train,],
control = tree.control(nobs = length(id_train),
mindev = my_mindev,
minsize = my_minsize))
# it has to overfit
plot(temp_tree_full)
# if maximum tree depth error
# change minsize = 2 to higher values and so do it with
# mindev
for (s in 2:my_max_size){
temp_tree_pruned = prune.tree(temp_tree_full, best = s, newdata = my_data[id_pruning,])
temp_predictions = predict(temp_tree_pruned, my_data[id_test,])
if(is_classification == TRUE){
# tree gives probabilities for each class
positive_index = which(colnames(temp_predictions) == 1)
temp_predictions = temp_predictions[,positive_index] > my_threshold %>% as.numeric
}
# s-1 because we start by size = 2
temp_metrics_array_cv[k,s-1,] = USED.Metrics(temp_prediction,
my_data$y[id_test],
weights = my_weights[id_test])
print(paste("tree size: ", s, collapse = ""))
}
rm(temp_tree_full)
rm(temp_tree_pruned)
rm(temp_predictions)
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = my_max_size - 1, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = my_max_size - 1, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
TREE_MAX_SIZE = 100
#' @param use_only_first_fold (bool): if yes fit the model on all except the first fold
#' and compute the metrics on that
#'
#' @param is_classification (bool): if TRUE adapt the metrics to classification problem
#' using the threshold
#' @param my_threshold (num): classification threshold used
#'
#' @param my_max_size (int): max size of the pruned tree
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvTree = function(n_k_fold,
my_id_list_cv,my_n_metrics,
my_metric_names,
my_data,
my_max_size = TREE_MAX_SIZE,
my_weights = 1,
my_mindev = 1e-05,
my_minsize = 2,
use_only_first_fold = FALSE,
is_classification = FALSE,
my_threshold = 0.5){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
if(use_only_first_fold == TRUE){
n_k_fold = 1
}
# switch to factor response
if(is_classification == TRUE){
my_data$y = factor(my_data$y)
}
# we use my_max_size - 1 because we start with size = 2
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, my_max_size - 1, my_n_metrics))
fold_indexes = 1:n_k_fold
for (k in fold_indexes) {
# k-th fold set is used as validation set
id_test = my_id_list_cv[[k]]
# one set among those remaining is chosen as the pruning set
# specifically we choose the one corresponding to the one with the minimum index
# among those not in the validation set
id_pruning = min(fold_indexes[-k])
# the remaining fold sets are used as training set
id_train = unlist(my_id_list_cv[-c(k, id_pruning)])
# full grown tree
temp_tree_full = tree(y ~.,
data = my_data[id_train,],
control = tree.control(nobs = length(id_train),
mindev = my_mindev,
minsize = my_minsize))
# it has to overfit
plot(temp_tree_full)
# if maximum tree depth error
# change minsize = 2 to higher values and so do it with
# mindev
for (s in 2:my_max_size){
temp_tree_pruned = prune.tree(temp_tree_full, best = s, newdata = my_data[id_pruning,])
temp_predictions = predict(temp_tree_pruned, my_data[id_test,])
if(is_classification == TRUE){
# tree gives probabilities for each class
positive_index = which(colnames(temp_predictions) == 1)
temp_predictions = temp_predictions[,positive_index] > my_threshold %>% as.numeric
}
# s-1 because we start by size = 2
temp_metrics_array_cv[k,s-1,] = USED.Metrics(temp_prediction,
my_data$y[id_test],
weights = my_weights[id_test])
print(paste("tree size: ", s, collapse = ""))
}
rm(temp_tree_full)
rm(temp_tree_pruned)
rm(temp_predictions)
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = my_max_size - 1, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = my_max_size - 1, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
tree_cv_metrics = ManualCvTree(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_metrics_functions = MY_USED_METRICS,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-04,
my_minsize = 5,
is_classification = TRUE,
my_threshold = MY_THRESHOLD,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
tree_cv_metrics = ManualCvTree(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-04,
my_minsize = 5,
is_classification = TRUE,
my_threshold = MY_THRESHOLD,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
#' @param use_only_first_fold (bool): if yes fit the model on all except the first fold
#' and compute the metrics on that
#'
#' @param is_classification (bool): if TRUE adapt the metrics to classification problem
#' using the threshold
#' @param my_threshold (num): classification threshold used
#'
#' @param my_max_size (int): max size of the pruned tree
#'
#' @return matrix of CV folds averaged errors for each parameter value and each loss function
ManualCvTree = function(n_k_fold,
my_id_list_cv,my_n_metrics,
my_metric_names,
my_data,
my_max_size = TREE_MAX_SIZE,
my_weights = 1,
my_mindev = 1e-05,
my_minsize = 2,
use_only_first_fold = FALSE,
is_classification = FALSE,
my_threshold = 0.5){
n_k_fold = length(my_id_list_cv)
my_n_metrics = length(my_metric_names)
if(use_only_first_fold == TRUE){
n_k_fold = 1
}
# switch to factor response
if(is_classification == TRUE){
my_data$y = factor(my_data$y)
}
# we use my_max_size - 1 because we start with size = 2
temp_metrics_array_cv = array(NA, dim = c(n_k_fold, my_max_size - 1, my_n_metrics))
fold_indexes = 1:n_k_fold
for (k in fold_indexes) {
# k-th fold set is used as validation set
id_test = my_id_list_cv[[k]]
# one set among those remaining is chosen as the pruning set
# specifically we choose the one corresponding to the one with the minimum index
# among those not in the validation set
id_pruning = min(fold_indexes[-k])
# the remaining fold sets are used as training set
id_train = unlist(my_id_list_cv[-c(k, id_pruning)])
# full grown tree
temp_tree_full = tree(y ~.,
data = my_data[id_train,],
control = tree.control(nobs = length(id_train),
mindev = my_mindev,
minsize = my_minsize))
# it has to overfit
plot(temp_tree_full)
# if maximum tree depth error
# change minsize = 2 to higher values and so do it with
# mindev
for (s in 2:my_max_size){
temp_tree_pruned = prune.tree(temp_tree_full, best = s, newdata = my_data[id_pruning,])
temp_predictions = predict(temp_tree_pruned, my_data[id_test,])
if(is_classification == TRUE){
# tree gives probabilities for each class
positive_index = which(colnames(temp_predictions) == 1)
temp_predictions = temp_predictions[,positive_index] > my_threshold %>% as.numeric
}
# s-1 because we start by size = 2
temp_metrics_array_cv[k,s-1,] = USED.Metrics(temp_predictions,
my_data$y[id_test],
weights = my_weights[id_test])
print(paste("tree size: ", s, collapse = ""))
}
rm(temp_tree_full)
rm(temp_tree_pruned)
rm(temp_predictions)
gc()
print(paste("fold ", k))
}
# averaged metrics matrix
cv_metrics = matrix(NA, nrow = my_max_size - 1, ncol = my_n_metrics)
# metrics standard deviations matrix
cv_metrics_se = matrix(NA, nrow = my_max_size - 1, ncol = my_n_metrics)
colnames(cv_metrics) = my_metric_names
colnames(cv_metrics_se) = my_metric_names
for (i in 1:my_n_metrics){
if(use_only_first_fold == FALSE){
cv_metrics[,i] = apply(temp_metrics_array_cv[,,i], 2, mean)
cv_metrics_se[,i] = apply(temp_metrics_array_cv[,,i], 2, sd)}
else{
cv_metrics[,i] = temp_metrics_array_cv[1,,i]
cv_metrics_se[,i] = 0
}
}
return(list("metrics" = cv_metrics,
"se" = cv_metrics_se))
}
tree_cv_metrics = ManualCvTree(my_id_list_cv = ID_CV_LIST,
my_metric_names = METRICS_NAMES,
my_data = sss,
my_max_size = TREE_MAX_SIZE,
my_weights = MY_WEIGHTS_sss,
my_mindev = 1e-04,
my_minsize = 5,
is_classification = TRUE,
my_threshold = MY_THRESHOLD,
use_only_first_fold = USE_ONLY_FIRST_FOLD)
tree_cv_metrics
